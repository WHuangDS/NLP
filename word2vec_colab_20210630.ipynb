{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_colab_20210630.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:17.732878Z",
          "iopub.status.busy": "2021-06-16T18:46:17.732329Z",
          "iopub.status.idle": "2021-06-16T18:46:17.734487Z",
          "shell.execute_reply": "2021-06-16T18:46:17.734025Z"
        },
        "id": "0nbI5DtDGw-i"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word2vec\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word2vec.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haJUNjSB60Kh"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99d4ky2lWFvn"
      },
      "source": [
        "Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.\n",
        "\n",
        "Note: This tutorial is based on [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) and\n",
        "[Distributed\n",
        "Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). It is not an exact implementation of the papers. Rather, it is intended to illustrate the key ideas.\n",
        "\n",
        "These papers proposed two methods for learning representations of words: \n",
        "\n",
        "*   **Continuous Bag-of-Words Model** which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
        "*   **Continuous Skip-gram Model** which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n",
        "\n",
        "\n",
        "You'll use the skip-gram approach in this tutorial. First, you'll explore skip-grams and other concepts using a single sentence for illustration. Next, you'll train your own Word2Vec model on a small dataset. This tutorial also contains code to export the trained embeddings and visualize them in the [TensorFlow Embedding Projector](http://projector.tensorflow.org/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP00WlaMWBZC"
      },
      "source": [
        "## Skip-gram and Negative Sampling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr2wjv0bW236"
      },
      "source": [
        "While a bag-of-words model predicts a word given the neighboring context, a skip-gram model predicts the context (or neighbors) of a word, given the word itself. The model is trained on skip-grams, which are n-grams that allow tokens to be skipped (see the diagram below for an example). The context of a word can be represented through a set of skip-gram pairs of `(target_word, context_word)` where `context_word` appears in the neighboring context of `target_word`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjc-McbaVTd"
      },
      "source": [
        "Consider the following sentence of 8 words.\n",
        "> The wide road shimmered in the hot sun. \n",
        "\n",
        "The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a `target_word` that can be considered `context word`. Take a look at this table of skip-grams for target words based on different window sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKE87IKT_YT8"
      },
      "source": [
        "Note: For this tutorial, a window size of *n* implies n words on each side with a total window span of 2*n+1 words across a word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsCwQ07E8mqU"
      },
      "source": [
        "![word2vec_skipgrams](images/word2vec_skipgram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK1gN1jwkMpU"
      },
      "source": [
        "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words *w<sub>1</sub>, w<sub>2</sub>, ... w<sub>T</sub>*, the objective can be written as the average log probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pILO_iAc84e-"
      },
      "source": [
        "![word2vec_skipgram_objective](images/word2vec_skipgram_objective.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsy6TUbtnz_K"
      },
      "source": [
        "where `c` is the size of the training context. The basic skip-gram formulation defines this probability using the softmax function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P81Qavbb9APd"
      },
      "source": [
        "![word2vec_full_softmax](images/word2vec_full_softmax.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axZvd-hhotVB"
      },
      "source": [
        "where *v* and *v<sup>'<sup>* are target and context vector representations of words and *W* is vocabulary size. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoLzxbqSpT6_"
      },
      "source": [
        "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words which is often large (10<sup>5</sup>-10<sup>7</sup>) terms. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5VWYtmFzHkU"
      },
      "source": [
        "The [Noise Contrastive Estimation](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modelling the word distribution, NCE loss can be [simplified](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) to use negative sampling. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTZBPf1RsOsg"
      },
      "source": [
        "The simplified negative sampling objective for a target word is to distinguish  the context word from *num_ns* negative samples drawn from noise distribution *P<sub>n</sub>(w)* of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and *num_ns* negative samples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl0rSfHjt6Mf"
      },
      "source": [
        "A negative sample is defined as a (target_word, context_word) pair such that the context_word does not appear in the `window_size` neighborhood of the target_word. For the example sentence, these are few potential negative samples (when `window_size` is 2).\n",
        "\n",
        "```\n",
        "(hot, shimmered)\n",
        "(wide, hot)\n",
        "(wide, sun)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq0q2uqbucFg"
      },
      "source": [
        "In the next section, you'll generate skip-grams and negative samples for a single sentence. You'll also learn about subsampling techniques and train a classification model for positive and negative training examples later in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4-Hpe1CH16"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:17.742757Z",
          "iopub.status.busy": "2021-06-16T18:46:17.742120Z",
          "iopub.status.idle": "2021-06-16T18:46:19.141796Z",
          "shell.execute_reply": "2021-06-16T18:46:19.141168Z"
        },
        "id": "RutaI-Tpev3T"
      },
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.145924Z",
          "iopub.status.busy": "2021-06-16T18:46:19.145340Z",
          "iopub.status.idle": "2021-06-16T18:46:19.148941Z",
          "shell.execute_reply": "2021-06-16T18:46:19.148514Z"
        },
        "id": "10pyUMFkGKVQ"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.152404Z",
          "iopub.status.busy": "2021-06-16T18:46:19.151837Z",
          "iopub.status.idle": "2021-06-16T18:46:19.153851Z",
          "shell.execute_reply": "2021-06-16T18:46:19.153404Z"
        },
        "id": "XkJ5299Tek6B"
      },
      "source": [
        "SEED = 42 \n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW-g5buCHwh3"
      },
      "source": [
        "### Vectorize an example sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8TfZIgoQrcP"
      },
      "source": [
        "Consider the following sentence:    \n",
        "`The wide road shimmered in the hot sun.`\n",
        "\n",
        "Tokenize the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.157824Z",
          "iopub.status.busy": "2021-06-16T18:46:19.157263Z",
          "iopub.status.idle": "2021-06-16T18:46:19.159935Z",
          "shell.execute_reply": "2021-06-16T18:46:19.160284Z"
        },
        "id": "bsl7jBzV6_KK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f580d5f-aecb-4c59-c258-25e4ee223b96"
      },
      "source": [
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "tokens = list(sentence.lower().split())\n",
        "print(len(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU-bs1XtThEw"
      },
      "source": [
        "Create a vocabulary to save mappings from tokens to integer indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.164396Z",
          "iopub.status.busy": "2021-06-16T18:46:19.163862Z",
          "iopub.status.idle": "2021-06-16T18:46:19.166343Z",
          "shell.execute_reply": "2021-06-16T18:46:19.165860Z"
        },
        "id": "UdYv1HJUQ8XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21f261f-04ec-4368-d281-89e9b056134f"
      },
      "source": [
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab['<pad>'] = 0  # add a padding token\n",
        "for token in tokens:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = index\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpuP43Dddasr"
      },
      "source": [
        "Create an inverse vocabulary to save mappings from integer indices to tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.169950Z",
          "iopub.status.busy": "2021-06-16T18:46:19.169420Z",
          "iopub.status.idle": "2021-06-16T18:46:19.171366Z",
          "shell.execute_reply": "2021-06-16T18:46:19.171737Z"
        },
        "id": "o9ULAJYtEvKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "610ca9fd-b615-44ba-cc1d-c485ce2c1817"
      },
      "source": [
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(inverse_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3qtuyxIRyii"
      },
      "source": [
        "Vectorize your sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.175260Z",
          "iopub.status.busy": "2021-06-16T18:46:19.174700Z",
          "iopub.status.idle": "2021-06-16T18:46:19.176579Z",
          "shell.execute_reply": "2021-06-16T18:46:19.176904Z"
        },
        "id": "CsB3-9uQQYyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a6701f-c135-4c3c-ff90-b99478ecf021"
      },
      "source": [
        "example_sequence = [vocab[word] for word in tokens]\n",
        "print(example_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 1, 6, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox1I28JRIOdM"
      },
      "source": [
        "### Generate skip-grams from one sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7NNKAmSiHvy"
      },
      "source": [
        "The `tf.keras.preprocessing.sequence` module provides useful functions that simplify data preparation for Word2Vec. You can use the `tf.keras.preprocessing.sequence.skipgrams` to generate skip-gram pairs from the `example_sequence` with a given `window_size` from tokens in the range `[0, vocab_size)`.\n",
        "\n",
        "Note: `negative_samples` is set to `0` here as batching negative samples generated by this function requires a bit of code. You will use another function to perform negative sampling in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.180833Z",
          "iopub.status.busy": "2021-06-16T18:46:19.180262Z",
          "iopub.status.idle": "2021-06-16T18:46:19.182490Z",
          "shell.execute_reply": "2021-06-16T18:46:19.182070Z"
        },
        "id": "USAJxW4RD7pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09347b9d-8927-4172-f6a1-fdee1330220d"
      },
      "source": [
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence,\n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "print(len(positive_skip_grams))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc9uhiMwY-AQ"
      },
      "source": [
        "Take a look at few positive skip-grams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:19.186150Z",
          "iopub.status.busy": "2021-06-16T18:46:19.185622Z",
          "iopub.status.idle": "2021-06-16T18:46:19.188098Z",
          "shell.execute_reply": "2021-06-16T18:46:19.187707Z"
        },
        "id": "SCnqEukIE9pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5600e311-88fe-4f5d-ceac-3fa82ff0ae0b"
      },
      "source": [
        "for target, context in positive_skip_grams[:20]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 3): (shimmered, road)\n",
            "(5, 6): (in, hot)\n",
            "(1, 6): (the, hot)\n",
            "(1, 7): (the, sun)\n",
            "(3, 2): (road, wide)\n",
            "(5, 1): (in, the)\n",
            "(4, 2): (shimmered, wide)\n",
            "(2, 3): (wide, road)\n",
            "(3, 4): (road, shimmered)\n",
            "(7, 1): (sun, the)\n",
            "(4, 1): (shimmered, the)\n",
            "(2, 1): (wide, the)\n",
            "(5, 3): (in, road)\n",
            "(5, 4): (in, shimmered)\n",
            "(1, 4): (the, shimmered)\n",
            "(6, 7): (hot, sun)\n",
            "(7, 6): (sun, hot)\n",
            "(2, 4): (wide, shimmered)\n",
            "(3, 5): (road, in)\n",
            "(1, 5): (the, in)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ua9PkMTISF0"
      },
      "source": [
        "### Negative sampling for one skip-gram "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esqn8WBfZnEK"
      },
      "source": [
        "The `skipgrams` function returns all positive skip-gram pairs by sliding over a given window span. To produce additional skip-gram pairs that would serve as negative samples for training, you need to sample random words from the vocabulary. Use the `tf.random.log_uniform_candidate_sampler` function to sample `num_ns` number of negative samples for a given target word in a window. You can call the function on one skip-grams's target word and pass the context word as true class to exclude it from being sampled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgH3aSvw3xTD"
      },
      "source": [
        "Key point: *num_ns* (number of negative samples per positive context word) between [5, 20] is [shown to work](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) best for smaller datasets, while *num_ns* between [2,5] suffices for larger datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.469014Z",
          "iopub.status.busy": "2021-06-16T18:46:20.468216Z",
          "iopub.status.idle": "2021-06-16T18:46:20.740633Z",
          "shell.execute_reply": "2021-06-16T18:46:20.740972Z"
        },
        "id": "m_LmdzqIGr5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fd1df9-122d-4a51-c907-b3c5ed522dd6"
      },
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "# Set the number of negative samples per positive context.\n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
        "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns,  # number of negative context words to sample\n",
        "    unique=True,  # all the negative samples should be unique\n",
        "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED,  # seed for reproducibility\n",
        "    name=\"negative_sampling\"  # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
            "['wide', 'the', 'shimmered', 'road']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MSxWCrLIalp"
      },
      "source": [
        "### Construct one training example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6uEWdj8vKKv"
      },
      "source": [
        "For a given positive `(target_word, context_word)` skip-gram, you now also have `num_ns` negative sampled context words that do not appear in the window size neighborhood of `target_word`. Batch the `1` positive `context_word` and `num_ns` negative context words into one tensor. This produces a set of positive skip-grams (labelled as `1`) and negative samples (labelled as `0`) for each target word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.746015Z",
          "iopub.status.busy": "2021-06-16T18:46:20.745447Z",
          "iopub.status.idle": "2021-06-16T18:46:20.748447Z",
          "shell.execute_reply": "2021-06-16T18:46:20.748032Z"
        },
        "id": "zSiZwifuLvHf"
      },
      "source": [
        "# Add a dimension so you can use concatenation (on the next step).\n",
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "\n",
        "# Concat positive context word with negative sampled words.\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label = tf.squeeze(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIJeoFCAwtXJ"
      },
      "source": [
        "Take a look at the context and the corresponding labels for the target word from the skip-gram example above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.752571Z",
          "iopub.status.busy": "2021-06-16T18:46:20.751804Z",
          "iopub.status.idle": "2021-06-16T18:46:20.756287Z",
          "shell.execute_reply": "2021-06-16T18:46:20.756608Z"
        },
        "id": "tzyCPCuZwmdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab04d24-2c10-47be-f500-e91a8a44662e"
      },
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target_index    : 4\n",
            "target_word     : shimmered\n",
            "context_indices : [3 2 1 4 3]\n",
            "context_words   : ['road', 'wide', 'the', 'shimmered', 'road']\n",
            "label           : [1 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBtTcUVQr8EO"
      },
      "source": [
        "A tuple of `(target, context, label)` tensors constitutes one training example for training your skip-gram negative sampling Word2Vec model. Notice that the target is of shape `(1,)` while the context and label are of shape `(1+num_ns,)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.760390Z",
          "iopub.status.busy": "2021-06-16T18:46:20.759770Z",
          "iopub.status.idle": "2021-06-16T18:46:20.762374Z",
          "shell.execute_reply": "2021-06-16T18:46:20.762696Z"
        },
        "id": "x-FwkR8jx9-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825a8357-f2c6-4c84-ba46-9bc33da80d5f"
      },
      "source": [
        "print(\"target  :\", target)\n",
        "print(\"context :\", context)\n",
        "print(\"label   :\", label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target  : tf.Tensor(4, shape=(), dtype=int32)\n",
            "context : tf.Tensor([3 2 1 4 3], shape=(5,), dtype=int64)\n",
            "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bRJIlow4Dlv"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWkuha0oykG5"
      },
      "source": [
        "This picture summarizes the procedure of generating training example from a sentence. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KlwdiAa9crJ"
      },
      "source": [
        "![word2vec_negative_sampling](images/word2vec_negative_sampling.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wmdO_MEIpaM"
      },
      "source": [
        "## Compile all steps into one function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLKwNAczHsKg"
      },
      "source": [
        "### Skip-gram Sampling table "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUUK3uDtFNFE"
      },
      "source": [
        "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as `the`, `is`, `on`) don't add much useful information  for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPtbv7zNP7Dx"
      },
      "source": [
        "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to  generate a word-frequency rank based probabilistic sampling table and pass it to `skipgrams` function. Take a look at the sampling probabilities for a `vocab_size` of 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.767393Z",
          "iopub.status.busy": "2021-06-16T18:46:20.766829Z",
          "iopub.status.idle": "2021-06-16T18:46:20.769241Z",
          "shell.execute_reply": "2021-06-16T18:46:20.768830Z"
        },
        "id": "Rn9zAnDccyRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098b875d-693c-4d4c-d03c-513238497fc4"
      },
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
        "print(sampling_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
            " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHvSptcPk5fp"
      },
      "source": [
        "`sampling_table[i]` denotes the probability of sampling the i-th most common word in a dataset. The function assumes a [Zipf's distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) of the word frequencies for sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRHMssMmHgH-"
      },
      "source": [
        "Key point: The `tf.random.log_uniform_candidate_sampler` already assumes that the vocabulary frequency follows a log-uniform (Zipf's) distribution. Using these distribution weighted sampling also helps approximate the Noise Contrastive Estimation (NCE) loss with simpler loss functions for training a negative sampling objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj--8RFK6fgW"
      },
      "source": [
        "### Generate training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy5hl4lQ0B2M"
      },
      "source": [
        "Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.775998Z",
          "iopub.status.busy": "2021-06-16T18:46:20.775393Z",
          "iopub.status.idle": "2021-06-16T18:46:20.777120Z",
          "shell.execute_reply": "2021-06-16T18:46:20.777446Z"
        },
        "id": "63INISDEX1Hu"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          seed=SEED,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=SEED,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shvPC8Ji2cMK"
      },
      "source": [
        "## Prepare training data for Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5mbZsZu6uKg"
      },
      "source": [
        "With an understanding of how to work with one sentence for a skip-gram negative sampling based Word2Vec model, you can proceed to generate training examples from a larger list of sentences!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFlikI6L26nh"
      },
      "source": [
        "### Download text corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEFavOgN98al"
      },
      "source": [
        "You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.782472Z",
          "iopub.status.busy": "2021-06-16T18:46:20.781924Z",
          "iopub.status.idle": "2021-06-16T18:46:20.983587Z",
          "shell.execute_reply": "2021-06-16T18:46:20.983147Z"
        },
        "id": "QFkitxzVVaAi"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOsbLq8a37dr"
      },
      "source": [
        "Read text from the file and take a look at the first few lines. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:20.987384Z",
          "iopub.status.busy": "2021-06-16T18:46:20.986824Z",
          "iopub.status.idle": "2021-06-16T18:46:20.994503Z",
          "shell.execute_reply": "2021-06-16T18:46:20.994892Z"
        },
        "id": "lfgnsUw3ofMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9d6434-3521-41fc-b81c-e76ef4e8b38e"
      },
      "source": [
        "with open(path_to_file) as f: \n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:100]:\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First Citizen:\n",
            "Soft! who comes here?\n",
            "\n",
            "Second Citizen:\n",
            "Worthy Menenius Agrippa; one that hath always loved\n",
            "the people.\n",
            "\n",
            "First Citizen:\n",
            "He's one honest enough: would all the rest were so!\n",
            "\n",
            "MENENIUS:\n",
            "What work's, my countrymen, in hand? where go you\n",
            "With bats and clubs? The matter? speak, I pray you.\n",
            "\n",
            "First Citizen:\n",
            "Our business is not unknown to the senate; they have\n",
            "had inkling this fortnight what we intend to do,\n",
            "which now we'll show 'em in deeds. They say poor\n",
            "suitors have strong breaths: they shall know we\n",
            "have strong arms too.\n",
            "\n",
            "MENENIUS:\n",
            "Why, masters, my good friends, mine honest neighbours,\n",
            "Will you undo yourselves?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTNZYqUs5C2V"
      },
      "source": [
        "Use the non empty lines to construct a `tf.data.TextLineDataset` object for next steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:21.000434Z",
          "iopub.status.busy": "2021-06-16T18:46:20.999883Z",
          "iopub.status.idle": "2021-06-16T18:46:21.036852Z",
          "shell.execute_reply": "2021-06-16T18:46:21.036404Z"
        },
        "id": "ViDrwy-HjAs9"
      },
      "source": [
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfsc88zE9upk"
      },
      "source": [
        "### Vectorize sentences from the corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfgZo8zR94KK"
      },
      "source": [
        "You can use the `TextVectorization` layer to vectorize sentences from the corpus. Learn more about using this layer in this [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a `custom_standardization function` that can be used in the TextVectorization layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:21.046382Z",
          "iopub.status.busy": "2021-06-16T18:46:21.040964Z",
          "iopub.status.idle": "2021-06-16T18:46:21.050072Z",
          "shell.execute_reply": "2021-06-16T18:46:21.050409Z"
        },
        "id": "2MlsXzo-ZlfK"
      },
      "source": [
        "# Now, create a custom standardization function to lowercase the text and\n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g92LuvnyBmz1"
      },
      "source": [
        "Call `adapt` on the text dataset to create vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:21.054199Z",
          "iopub.status.busy": "2021-06-16T18:46:21.053611Z",
          "iopub.status.idle": "2021-06-16T18:46:22.315263Z",
          "shell.execute_reply": "2021-06-16T18:46:22.315717Z"
        },
        "id": "seZau_iYMPFT"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMiONR6OqwHw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg2z7eeHMnH-"
      },
      "source": [
        "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `get_vocabulary()`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:22.320243Z",
          "iopub.status.busy": "2021-06-16T18:46:22.319670Z",
          "iopub.status.idle": "2021-06-16T18:46:22.325924Z",
          "shell.execute_reply": "2021-06-16T18:46:22.326257Z"
        },
        "id": "jgw9pTA7MRaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803359b7-c1a2-4a1c-9fea-382397bc2481"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOQ30Tx6KA2G"
      },
      "source": [
        "The vectorize_layer can now be used to generate vectors for each element in the `text_ds`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:22.330310Z",
          "iopub.status.busy": "2021-06-16T18:46:22.329737Z",
          "iopub.status.idle": "2021-06-16T18:46:22.371833Z",
          "shell.execute_reply": "2021-06-16T18:46:22.372174Z"
        },
        "id": "yUVYrDp0araQ"
      },
      "source": [
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YyH_SYzB72p"
      },
      "source": [
        "### Obtain sequences from the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFUQLX0_KaRC"
      },
      "source": [
        "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a Word2Vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples. \n",
        "\n",
        "Note: Since the `generate_training_data()` defined earlier uses non-TF python/numpy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:22.376519Z",
          "iopub.status.busy": "2021-06-16T18:46:22.375960Z",
          "iopub.status.idle": "2021-06-16T18:46:24.616943Z",
          "shell.execute_reply": "2021-06-16T18:46:24.616512Z"
        },
        "id": "sGXoOh9y11pM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffad46cb-41d3-40ba-ad29-65d55d34b151"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDc4riukLTqg"
      },
      "source": [
        "Take a look at few examples from `sequences`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:24.621930Z",
          "iopub.status.busy": "2021-06-16T18:46:24.621248Z",
          "iopub.status.idle": "2021-06-16T18:46:24.623974Z",
          "shell.execute_reply": "2021-06-16T18:46:24.623566Z"
        },
        "id": "WZf1RIbB2Dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce34497a-0b35-4356-fc3c-8210c529623f"
      },
      "source": [
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
            "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDzSOjNwCWNh"
      },
      "source": [
        "### Generate training examples from sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehvYr-nEKyY"
      },
      "source": [
        "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data()` function defined earlier to generate training examples for the Word2Vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be same, representing the total number of training examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:24.628021Z",
          "iopub.status.busy": "2021-06-16T18:46:24.627471Z",
          "iopub.status.idle": "2021-06-16T18:46:50.497508Z",
          "shell.execute_reply": "2021-06-16T18:46:50.497083Z"
        },
        "id": "44DJ22M6nX5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0675994d-ccd4-482d-ef33-b2b6fac244a1"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 32777/32777 [00:09<00:00, 3570.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "65317 65317 65317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97PqsusOFEpc"
      },
      "source": [
        "### Configure the dataset for performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jnFVySViQTj"
      },
      "source": [
        "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your Word2Vec model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:50.541090Z",
          "iopub.status.busy": "2021-06-16T18:46:50.525637Z",
          "iopub.status.idle": "2021-06-16T18:46:54.636851Z",
          "shell.execute_reply": "2021-06-16T18:46:54.637232Z"
        },
        "id": "nbu8PxPSnVY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e951b27d-8a15-4ec4-aeb7-4666f690206f"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyrNX6Fs6K3F"
      },
      "source": [
        "Add `cache()` and `prefetch()` to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:54.641103Z",
          "iopub.status.busy": "2021-06-16T18:46:54.640562Z",
          "iopub.status.idle": "2021-06-16T18:46:54.643317Z",
          "shell.execute_reply": "2021-06-16T18:46:54.643689Z"
        },
        "id": "Y5Ueg6bcFPVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f049b672-ef4a-464a-dc43-fdf8a0af53e4"
      },
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S-CmUMszyEf"
      },
      "source": [
        "## Model and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQFqaBMPwBqC"
      },
      "source": [
        "The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc7kTbiwD9sy"
      },
      "source": [
        "### Subclassed Word2Vec Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvr9pM1G1sQN"
      },
      "source": [
        "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your Word2Vec model with the following layers:\n",
        "\n",
        "\n",
        "* `target_embedding`: A `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
        "* `context_embedding`: Another `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
        "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
        "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
        "\n",
        "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAwuIqqw7-7"
      },
      "source": [
        "Key point: The `target_embedding` and `context_embedding` layers can be shared as well. You could also use a concatenation of both embeddings as the final Word2Vec embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:54.649919Z",
          "iopub.status.busy": "2021-06-16T18:46:54.649369Z",
          "iopub.status.idle": "2021-06-16T18:46:54.651279Z",
          "shell.execute_reply": "2021-06-16T18:46:54.650880Z"
        },
        "id": "i9ec-sS6xd8Z"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3, 2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = self.dots([context_emb, word_emb])\n",
        "    return self.flatten(dots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RLKz9LFECXu"
      },
      "source": [
        "### Define loss function and compile model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Md-9QanqBM"
      },
      "source": [
        "For simplicity, you can use `tf.keras.losses.CategoricalCrossEntropy` as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:\n",
        "\n",
        "``` python\n",
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "```\n",
        "\n",
        "It's time to build your model! Instantiate your Word2Vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:54.655782Z",
          "iopub.status.busy": "2021-06-16T18:46:54.655183Z",
          "iopub.status.idle": "2021-06-16T18:46:54.836295Z",
          "shell.execute_reply": "2021-06-16T18:46:54.836660Z"
        },
        "id": "ekQg_KbWnnmQ"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3MUMrluqNX2"
      },
      "source": [
        "Also define a callback to log training statistics for tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:54.840621Z",
          "iopub.status.busy": "2021-06-16T18:46:54.840073Z",
          "iopub.status.idle": "2021-06-16T18:46:55.090867Z",
          "shell.execute_reply": "2021-06-16T18:46:55.091248Z"
        },
        "id": "9d-ftBCeEZIR"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5wEBotlGZ7B"
      },
      "source": [
        "Train the model with `dataset` prepared above for some number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:46:55.095911Z",
          "iopub.status.busy": "2021-06-16T18:46:55.095275Z",
          "iopub.status.idle": "2021-06-16T18:47:03.076585Z",
          "shell.execute_reply": "2021-06-16T18:47:03.076071Z"
        },
        "id": "gmC1BJalEZIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49162cb7-e837-4b19-fc25-1aae7d38d22b"
      },
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 22ms/step - loss: 1.6083 - accuracy: 0.2282\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 1.5894 - accuracy: 0.5563\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 1.5423 - accuracy: 0.6125\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 1.4592 - accuracy: 0.5835\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 1.3599 - accuracy: 0.5847\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 1.2623 - accuracy: 0.6078\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 1.1719 - accuracy: 0.6404\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 1.0886 - accuracy: 0.6743\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 1.0115 - accuracy: 0.7055\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.9399 - accuracy: 0.7357\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.8735 - accuracy: 0.7607\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.8121 - accuracy: 0.7841\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.7555 - accuracy: 0.8032\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.7035 - accuracy: 0.8202\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.6557 - accuracy: 0.8353\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.6121 - accuracy: 0.8499\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.5722 - accuracy: 0.8626\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.5359 - accuracy: 0.8737\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 19ms/step - loss: 0.5028 - accuracy: 0.8829\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 0.4726 - accuracy: 0.8921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c0acba190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wze38jG57XvZ"
      },
      "source": [
        "Tensorboard now shows the Word2Vec model's accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22E9eqS55rgz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "2441a399-7b97-4366-c43c-9eca4f1b5900"
      },
      "source": [
        "#docs_infra: no_execute\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1256), started 0:07:03 ago. (Use '!kill 1256' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awF3iRQCZOLj"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/word2vec_tensorboard.png\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaDW2tIIz8fL"
      },
      "source": [
        "## Embedding lookup and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp5rv01WG2YA"
      },
      "source": [
        "Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:47:03.080624Z",
          "iopub.status.busy": "2021-06-16T18:47:03.080091Z",
          "iopub.status.idle": "2021-06-16T18:47:03.088754Z",
          "shell.execute_reply": "2021-06-16T18:47:03.089099Z"
        },
        "id": "_Uamp1YH8RzU"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWzdmUzS8Sl4"
      },
      "source": [
        "Create and save the vectors and metadata file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:47:03.093632Z",
          "iopub.status.busy": "2021-06-16T18:47:03.093059Z",
          "iopub.status.idle": "2021-06-16T18:47:03.415093Z",
          "shell.execute_reply": "2021-06-16T18:47:03.414464Z"
        },
        "id": "VLIahl9s53XT"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T8KcThhIU8-"
      },
      "source": [
        "Download the `vectors.tsv` and `metadata.tsv` to analyze the obtained embeddings in the [Embedding Projector](https://projector.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-16T18:47:03.419205Z",
          "iopub.status.busy": "2021-06-16T18:47:03.418655Z",
          "iopub.status.idle": "2021-06-16T18:47:03.420420Z",
          "shell.execute_reply": "2021-06-16T18:47:03.420754Z"
        },
        "id": "lUsjQOKMIV2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "589bfa0a-47fe-4648-c577-c1c3c47ed2b9"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d62b6973-8f83-430e-bc20-323e9bce1c2e\", \"vectors.tsv\", 6124101)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e460e050-1af3-4a9f-a140-21f05b77508e\", \"metadata.tsv\", 28737)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS_uMeMw3Xpj"
      },
      "source": [
        "## Next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSgAZpwF5xF_"
      },
      "source": [
        "This tutorial has shown you how to implement a skip-gram Word2Vec model with negative sampling from scratch and visualize the obtained word embeddings.\n",
        "\n",
        "* To learn more about word vectors and their mathematical representations, refer to these [notes](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf).\n",
        "\n",
        "* To learn more about advanced text processing, read the [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer) tutorial.\n",
        "\n",
        "* If youre interested in pre-trained embedding models, you may also be interested in [Exploring the TF-Hub CORD-19 Swivel Embeddings](https://www.tensorflow.org/hub/tutorials/cord_19_embeddings_keras), or the [Multilingual Universal Sentence Encoder](https://www.tensorflow.org/hub/tutorials/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder)\n",
        "\n",
        "* You may also like to train the model on a new dataset (there are many available in [TensorFlow Datasets](https://www.tensorflow.org/datasets)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV7GkXYMUzqo"
      },
      "source": [
        "window_size1=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YLAZxKIJqRh",
        "outputId": "3db8ae7e-6a4d-4d4e-d3ae-02d8d35fa07c"
      },
      "source": [
        "targets1, contexts1, labels1 = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=window_size1, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 32777/32777 [00:14<00:00, 2304.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "65317 65317 65317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uteVb8tJqVK",
        "outputId": "50e1c1fe-ba21-46d5-b0f4-ba46ff0ee3f6"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(((targets1, contexts1), labels1))\n",
        "dataset1 = dataset1.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-nIHSwRJqYF",
        "outputId": "a8ebc2c7-59c2-4a52-9685-c5759808b7a3"
      },
      "source": [
        "dataset1 = dataset1.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JunjhFiNJqa3"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec1 = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jiq3HPEtJqdS",
        "outputId": "f4d1478d-4eae-413e-c280-5a7d6937aa82"
      },
      "source": [
        "word2vec1.fit(dataset1, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "106/106 [==============================] - 3s 21ms/step - loss: 1.6070 - accuracy: 0.2359\n",
            "Epoch 2/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 1.5680 - accuracy: 0.4338\n",
            "Epoch 3/20\n",
            "106/106 [==============================] - 2s 18ms/step - loss: 1.4892 - accuracy: 0.4123\n",
            "Epoch 4/20\n",
            "106/106 [==============================] - 2s 18ms/step - loss: 1.4070 - accuracy: 0.4488\n",
            "Epoch 5/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 1.3268 - accuracy: 0.4984\n",
            "Epoch 6/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 1.2477 - accuracy: 0.5478\n",
            "Epoch 7/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 1.1705 - accuracy: 0.5955\n",
            "Epoch 8/20\n",
            "106/106 [==============================] - 2s 18ms/step - loss: 1.0962 - accuracy: 0.6371\n",
            "Epoch 9/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 1.0255 - accuracy: 0.6748\n",
            "Epoch 10/20\n",
            "106/106 [==============================] - 2s 18ms/step - loss: 0.9589 - accuracy: 0.7049\n",
            "Epoch 11/20\n",
            "106/106 [==============================] - 2s 18ms/step - loss: 0.8968 - accuracy: 0.7310\n",
            "Epoch 12/20\n",
            "106/106 [==============================] - 2s 18ms/step - loss: 0.8393 - accuracy: 0.7534\n",
            "Epoch 13/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 0.7863 - accuracy: 0.7741\n",
            "Epoch 14/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 0.7377 - accuracy: 0.7923\n",
            "Epoch 15/20\n",
            "106/106 [==============================] - 2s 18ms/step - loss: 0.6932 - accuracy: 0.8076\n",
            "Epoch 16/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 0.6526 - accuracy: 0.8212\n",
            "Epoch 17/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 0.6156 - accuracy: 0.8332\n",
            "Epoch 18/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 0.5819 - accuracy: 0.8442\n",
            "Epoch 19/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 0.5512 - accuracy: 0.8541\n",
            "Epoch 20/20\n",
            "106/106 [==============================] - 2s 17ms/step - loss: 0.5231 - accuracy: 0.8628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c03884e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "MteePwZxJqgC",
        "outputId": "de358803-71ae-4908-9083-578d31464a40"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1256), started 0:08:08 ago. (Use '!kill 1256' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyOXYAM8Jqi3"
      },
      "source": [
        "window_size2 = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcCVF4XutE6p",
        "outputId": "9efb70ed-423e-4324-b01a-655a4aab61ae"
      },
      "source": [
        "targets2, contexts2, labels2 = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=window_size2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets2), len(contexts2), len(labels2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 32777/32777 [00:15<00:00, 2182.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "123341 123341 123341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6RzhFk0tE9F",
        "outputId": "a3e578df-7ad0-4b2f-b9f5-0d57a766d5d6"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(((targets2, contexts2), labels2))\n",
        "dataset2 = dataset2.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndROO8f0tE_6",
        "outputId": "641215ba-807c-45c5-bd6c-08ea0ff15e22"
      },
      "source": [
        "dataset2 = dataset2.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JcLhQystFCj"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec2 = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2mHDdzZuOuJ",
        "outputId": "ec5befc8-fdf4-408d-c2ed-f4a9d272055a"
      },
      "source": [
        "word2vec2.fit(dataset2, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "120/120 [==============================] - 3s 21ms/step - loss: 1.6062 - accuracy: 0.2363\n",
            "Epoch 2/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.5589 - accuracy: 0.4051\n",
            "Epoch 3/20\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 1.4779 - accuracy: 0.3939\n",
            "Epoch 4/20\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 1.3995 - accuracy: 0.4390\n",
            "Epoch 5/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.3219 - accuracy: 0.4911\n",
            "Epoch 6/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.2447 - accuracy: 0.5415\n",
            "Epoch 7/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1690 - accuracy: 0.5878\n",
            "Epoch 8/20\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 1.0957 - accuracy: 0.6281\n",
            "Epoch 9/20\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 1.0259 - accuracy: 0.6652\n",
            "Epoch 10/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9601 - accuracy: 0.6972\n",
            "Epoch 11/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8987 - accuracy: 0.7235\n",
            "Epoch 12/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8418 - accuracy: 0.7467\n",
            "Epoch 13/20\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 0.7893 - accuracy: 0.7669\n",
            "Epoch 14/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7412 - accuracy: 0.7854\n",
            "Epoch 15/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6972 - accuracy: 0.8011\n",
            "Epoch 16/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6570 - accuracy: 0.8152\n",
            "Epoch 17/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6203 - accuracy: 0.8279\n",
            "Epoch 18/20\n",
            "120/120 [==============================] - 2s 17ms/step - loss: 0.5868 - accuracy: 0.8393\n",
            "Epoch 19/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.5563 - accuracy: 0.8494\n",
            "Epoch 20/20\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.5285 - accuracy: 0.8580\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c01af5a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMKsCmHWuOwl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "839becb0-7ed8-4627-86fb-96b4ece673d3"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1256), started 0:09:17 ago. (Use '!kill 1256' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EszixtvxuOz3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFsOocxjc4dG"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data1(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels, positive_skip_grams_list= [], [], [],[]\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "    \n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "      \n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "      #positive_skip_grams_list.append(positive_skip_grams)\n",
        "      positive_skip_grams_list=positive_skip_grams_list+positive_skip_grams\n",
        "\n",
        "  return targets, contexts, labels,positive_skip_grams_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnfbmgyjd50z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLS7m6G-c4f5",
        "outputId": "25c7f91a-df5d-407f-ecdd-9aacba1f72b7"
      },
      "source": [
        "targets21, contexts21, labels21,positive_skip_grams_list21 = generate_training_data1(\n",
        "    sequences=sequences, \n",
        "    window_size=window_size2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets21), len(contexts21), len(labels21))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 32777/32777 [12:51<00:00, 42.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "122301 122301 122301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9lImziYdoKB",
        "outputId": "59645670-49a0-4ce8-b117-bd0901efd175"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset21 = tf.data.Dataset.from_tensor_slices(((targets21, contexts21), labels21))\n",
        "dataset21 = dataset21.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset21)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKakahwydoSC",
        "outputId": "f4f507eb-3382-4c38-9802-99dfecb59e9d"
      },
      "source": [
        "dataset21 = dataset21.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3SLjIBhdoY3"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec21 = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec21.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vqt9feNebJm",
        "outputId": "3d87e517-83ae-41a9-8fba-eb25d4bddcb7"
      },
      "source": [
        "word2vec21.fit(dataset21, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "119/119 [==============================] - 3s 20ms/step - loss: 1.6064 - accuracy: 0.2332\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 1.5606 - accuracy: 0.4078\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 1.4813 - accuracy: 0.3965\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 1.4043 - accuracy: 0.4388\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 1.3270 - accuracy: 0.4913\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 1.2491 - accuracy: 0.5423\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 1.1725 - accuracy: 0.5887\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 1.0986 - accuracy: 0.6311\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 1.0284 - accuracy: 0.6662\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 0.9625 - accuracy: 0.6958\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.9011 - accuracy: 0.7222\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.8443 - accuracy: 0.7449\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.7920 - accuracy: 0.7652\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.7440 - accuracy: 0.7837\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.7001 - accuracy: 0.7995\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 2s 17ms/step - loss: 0.6600 - accuracy: 0.8133\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.6234 - accuracy: 0.8262\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.5900 - accuracy: 0.8370\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.5595 - accuracy: 0.8467\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 2s 18ms/step - loss: 0.5317 - accuracy: 0.8555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1bfbf47690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "NQm7WjaaebMN",
        "outputId": "639832a7-04d5-4e62-effe-cfe1a478278c"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1256), started 0:23:03 ago. (Use '!kill 1256' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYw1qOTkefxF",
        "outputId": "385d67cc-1173-4f03-edcf-a81772fd9db0"
      },
      "source": [
        "for target212, context212 in positive_skip_grams_list21[:1000]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target212]}, {inverse_vocab[context212]})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, [3 2 1 4 3]): (further, hear)\n",
            "(4, [3 2 1 4 3]): (further, we)\n",
            "(4, [3 2 1 4 3]): (further, any)\n",
            "(4, [3 2 1 4 3]): (further, me)\n",
            "(4, [3 2 1 4 3]): (further, proceed)\n",
            "(4, [3 2 1 4 3]): (further, speak)\n",
            "(4, [3 2 1 4 3]): (further, before)\n",
            "(4, [3 2 1 4 3]): (further, hear)\n",
            "(4, [3 2 1 4 3]): (further, we)\n",
            "(4, [3 2 1 4 3]): (further, any)\n",
            "(4, [3 2 1 4 3]): (further, me)\n",
            "(4, [3 2 1 4 3]): (further, proceed)\n",
            "(4, [3 2 1 4 3]): (further, speak)\n",
            "(4, [3 2 1 4 3]): (further, before)\n",
            "(4, [3 2 1 4 3]): (further, hear)\n",
            "(4, [3 2 1 4 3]): (further, we)\n",
            "(4, [3 2 1 4 3]): (further, any)\n",
            "(4, [3 2 1 4 3]): (further, me)\n",
            "(4, [3 2 1 4 3]): (further, proceed)\n",
            "(4, [3 2 1 4 3]): (further, speak)\n",
            "(4, [3 2 1 4 3]): (further, before)\n",
            "(4, [3 2 1 4 3]): (further, hear)\n",
            "(4, [3 2 1 4 3]): (further, we)\n",
            "(4, [3 2 1 4 3]): (further, any)\n",
            "(4, [3 2 1 4 3]): (further, me)\n",
            "(4, [3 2 1 4 3]): (further, proceed)\n",
            "(4, [3 2 1 4 3]): (further, speak)\n",
            "(4, [3 2 1 4 3]): (further, before)\n",
            "(4, [3 2 1 4 3]): (further, hear)\n",
            "(4, [3 2 1 4 3]): (further, we)\n",
            "(4, [3 2 1 4 3]): (further, any)\n",
            "(4, [3 2 1 4 3]): (further, me)\n",
            "(4, [3 2 1 4 3]): (further, proceed)\n",
            "(4, [3 2 1 4 3]): (further, speak)\n",
            "(4, [3 2 1 4 3]): (further, before)\n",
            "(4, [3 2 1 4 3]): (further, hear)\n",
            "(4, [3 2 1 4 3]): (further, we)\n",
            "(4, [3 2 1 4 3]): (further, any)\n",
            "(4, [3 2 1 4 3]): (further, me)\n",
            "(4, [3 2 1 4 3]): (further, proceed)\n",
            "(4, [3 2 1 4 3]): (further, speak)\n",
            "(4, [3 2 1 4 3]): (further, before)\n",
            "(4, [3 2 1 4 3]): (further, hear)\n",
            "(4, [3 2 1 4 3]): (further, we)\n",
            "(4, [3 2 1 4 3]): (further, any)\n",
            "(4, [3 2 1 4 3]): (further, me)\n",
            "(4, [3 2 1 4 3]): (further, proceed)\n",
            "(4, [3 2 1 4 3]): (further, speak)\n",
            "(4, [3 2 1 4 3]): (further, before)\n",
            "(4, [3 2 1 4 3]): (famish, die)\n",
            "(4, [3 2 1 4 3]): (famish, than)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, rather)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, die)\n",
            "(4, [3 2 1 4 3]): (famish, than)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, rather)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, die)\n",
            "(4, [3 2 1 4 3]): (famish, than)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, rather)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, die)\n",
            "(4, [3 2 1 4 3]): (famish, than)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, rather)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, die)\n",
            "(4, [3 2 1 4 3]): (famish, than)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, rather)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (corn, well)\n",
            "(4, [3 2 1 4 3]): (corn, him)\n",
            "(4, [3 2 1 4 3]): (corn, kill)\n",
            "(4, [3 2 1 4 3]): (corn, our)\n",
            "(4, [3 2 1 4 3]): (corn, and)\n",
            "(4, [3 2 1 4 3]): (corn, at)\n",
            "(4, [3 2 1 4 3]): (corn, have)\n",
            "(4, [3 2 1 4 3]): (corn, well)\n",
            "(4, [3 2 1 4 3]): (corn, him)\n",
            "(4, [3 2 1 4 3]): (corn, kill)\n",
            "(4, [3 2 1 4 3]): (corn, our)\n",
            "(4, [3 2 1 4 3]): (corn, and)\n",
            "(4, [3 2 1 4 3]): (corn, at)\n",
            "(4, [3 2 1 4 3]): (corn, have)\n",
            "(4, [3 2 1 4 3]): (corn, well)\n",
            "(4, [3 2 1 4 3]): (corn, him)\n",
            "(4, [3 2 1 4 3]): (corn, kill)\n",
            "(4, [3 2 1 4 3]): (corn, our)\n",
            "(4, [3 2 1 4 3]): (corn, and)\n",
            "(4, [3 2 1 4 3]): (corn, at)\n",
            "(4, [3 2 1 4 3]): (corn, have)\n",
            "(4, [3 2 1 4 3]): (corn, well)\n",
            "(4, [3 2 1 4 3]): (corn, him)\n",
            "(4, [3 2 1 4 3]): (corn, kill)\n",
            "(4, [3 2 1 4 3]): (corn, our)\n",
            "(4, [3 2 1 4 3]): (corn, and)\n",
            "(4, [3 2 1 4 3]): (corn, at)\n",
            "(4, [3 2 1 4 3]): (corn, have)\n",
            "(4, [3 2 1 4 3]): (corn, well)\n",
            "(4, [3 2 1 4 3]): (corn, him)\n",
            "(4, [3 2 1 4 3]): (corn, kill)\n",
            "(4, [3 2 1 4 3]): (corn, our)\n",
            "(4, [3 2 1 4 3]): (corn, and)\n",
            "(4, [3 2 1 4 3]): (corn, at)\n",
            "(4, [3 2 1 4 3]): (corn, have)\n",
            "(4, [3 2 1 4 3]): (corn, well)\n",
            "(4, [3 2 1 4 3]): (corn, him)\n",
            "(4, [3 2 1 4 3]): (corn, kill)\n",
            "(4, [3 2 1 4 3]): (corn, our)\n",
            "(4, [3 2 1 4 3]): (corn, and)\n",
            "(4, [3 2 1 4 3]): (corn, at)\n",
            "(4, [3 2 1 4 3]): (corn, have)\n",
            "(4, [3 2 1 4 3]): (corn, well)\n",
            "(4, [3 2 1 4 3]): (corn, him)\n",
            "(4, [3 2 1 4 3]): (corn, kill)\n",
            "(4, [3 2 1 4 3]): (corn, our)\n",
            "(4, [3 2 1 4 3]): (corn, and)\n",
            "(4, [3 2 1 4 3]): (corn, at)\n",
            "(4, [3 2 1 4 3]): (corn, have)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, be)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (more, be)\n",
            "(4, [3 2 1 4 3]): (away, be)\n",
            "(4, [3 2 1 4 3]): (more, talking)\n",
            "(4, [3 2 1 4 3]): (away, away)\n",
            "(4, [3 2 1 4 3]): (talking, it)\n",
            "(4, [3 2 1 4 3]): (more, let)\n",
            "(4, [3 2 1 4 3]): (talking, done)\n",
            "(4, [3 2 1 4 3]): (away, it)\n",
            "(4, [3 2 1 4 3]): (more, ont)\n",
            "(4, [3 2 1 4 3]): (more, it)\n",
            "(4, [3 2 1 4 3]): (more, no)\n",
            "(4, [3 2 1 4 3]): (away, let)\n",
            "(4, [3 2 1 4 3]): (away, ont)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (away, done)\n",
            "(4, [3 2 1 4 3]): (authority, us)\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, would)\n",
            "(4, [3 2 1 4 3]): (authority, us)\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, would)\n",
            "(4, [3 2 1 4 3]): (authority, us)\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, would)\n",
            "(4, [3 2 1 4 3]): (authority, us)\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, would)\n",
            "(4, [3 2 1 4 3]): (authority, us)\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, would)\n",
            "(4, [3 2 1 4 3]): (authority, us)\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, would)\n",
            "(4, [3 2 1 4 3]): (yield, us)\n",
            "(4, [3 2 1 4 3]): (yield, but)\n",
            "(4, [3 2 1 4 3]): (yield, [UNK])\n",
            "(4, [3 2 1 4 3]): (yield, would)\n",
            "(4, [3 2 1 4 3]): (yield, the)\n",
            "(4, [3 2 1 4 3]): (yield, while)\n",
            "(4, [3 2 1 4 3]): (yield, us)\n",
            "(4, [3 2 1 4 3]): (yield, but)\n",
            "(4, [3 2 1 4 3]): (yield, [UNK])\n",
            "(4, [3 2 1 4 3]): (yield, would)\n",
            "(4, [3 2 1 4 3]): (yield, the)\n",
            "(4, [3 2 1 4 3]): (yield, while)\n",
            "(4, [3 2 1 4 3]): (yield, us)\n",
            "(4, [3 2 1 4 3]): (yield, but)\n",
            "(4, [3 2 1 4 3]): (yield, [UNK])\n",
            "(4, [3 2 1 4 3]): (yield, would)\n",
            "(4, [3 2 1 4 3]): (yield, the)\n",
            "(4, [3 2 1 4 3]): (yield, while)\n",
            "(4, [3 2 1 4 3]): (yield, us)\n",
            "(4, [3 2 1 4 3]): (yield, but)\n",
            "(4, [3 2 1 4 3]): (yield, [UNK])\n",
            "(4, [3 2 1 4 3]): (yield, would)\n",
            "(4, [3 2 1 4 3]): (yield, the)\n",
            "(4, [3 2 1 4 3]): (yield, while)\n",
            "(4, [3 2 1 4 3]): (yield, us)\n",
            "(4, [3 2 1 4 3]): (yield, but)\n",
            "(4, [3 2 1 4 3]): (yield, [UNK])\n",
            "(4, [3 2 1 4 3]): (yield, would)\n",
            "(4, [3 2 1 4 3]): (yield, the)\n",
            "(4, [3 2 1 4 3]): (yield, while)\n",
            "(4, [3 2 1 4 3]): (yield, us)\n",
            "(4, [3 2 1 4 3]): (yield, but)\n",
            "(4, [3 2 1 4 3]): (yield, [UNK])\n",
            "(4, [3 2 1 4 3]): (yield, would)\n",
            "(4, [3 2 1 4 3]): (yield, the)\n",
            "(4, [3 2 1 4 3]): (yield, while)\n",
            "(4, [3 2 1 4 3]): (sufferance, them)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, to)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (sufferance, gain)\n",
            "(4, [3 2 1 4 3]): (sufferance, them)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, to)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (sufferance, gain)\n",
            "(4, [3 2 1 4 3]): (sufferance, them)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, to)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (sufferance, gain)\n",
            "(4, [3 2 1 4 3]): (sufferance, them)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, to)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (sufferance, gain)\n",
            "(4, [3 2 1 4 3]): (sufferance, them)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, to)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (sufferance, gain)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, bread)\n",
            "(4, [3 2 1 4 3]): (thirst, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, bread)\n",
            "(4, [3 2 1 4 3]): (thirst, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, bread)\n",
            "(4, [3 2 1 4 3]): (thirst, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, bread)\n",
            "(4, [3 2 1 4 3]): (thirst, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, bread)\n",
            "(4, [3 2 1 4 3]): (thirst, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (thirst, bread)\n",
            "(4, [3 2 1 4 3]): (thirst, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizen, second)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (a, very)\n",
            "(4, [3 2 1 4 3]): (hes, the)\n",
            "(4, [3 2 1 4 3]): (hes, to)\n",
            "(4, [3 2 1 4 3]): (a, him)\n",
            "(4, [3 2 1 4 3]): (hes, him)\n",
            "(4, [3 2 1 4 3]): (hes, dog)\n",
            "(4, [3 2 1 4 3]): (a, hes)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (a, the)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (a, first)\n",
            "(4, [3 2 1 4 3]): (hes, first)\n",
            "(4, [3 2 1 4 3]): (a, against)\n",
            "(4, [3 2 1 4 3]): (dog, [UNK])\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (a, [UNK])\n",
            "(4, [3 2 1 4 3]): (a, to)\n",
            "(4, [3 2 1 4 3]): (hes, against)\n",
            "(4, [3 2 1 4 3]): (dog, him)\n",
            "(4, [3 2 1 4 3]): (dog, hes)\n",
            "(4, [3 2 1 4 3]): (a, dog)\n",
            "(4, [3 2 1 4 3]): (dog, first)\n",
            "(4, [3 2 1 4 3]): (hes, very)\n",
            "(4, [3 2 1 4 3]): (hes, a)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa4Q8ZhBe61G",
        "outputId": "37afe9d1-323d-4796-a298-de28afbd3459"
      },
      "source": [
        "positive_skip_grams_list21[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [3690, 200]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p-YZxvafZez",
        "outputId": "867c9dc3-3a9d-420a-b3ef-0d4b99833426"
      },
      "source": [
        "len(positive_skip_grams_list21)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1309251"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62TzZMHZgLUl",
        "outputId": "3d1e9684-f13e-409a-b9b7-e8efdb7ca9a2"
      },
      "source": [
        "from tkinter import _flatten\n",
        "print(_flatten(positive_skip_grams_list21[:50]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(673, 125, 673, 36, 673, 144, 673, 16, 673, 982, 673, 106, 673, 138, 673, 125, 673, 36, 673, 144, 673, 16, 673, 982, 673, 106, 673, 138, 673, 125, 673, 36, 673, 144, 673, 16, 673, 982, 673, 106, 673, 138, 673, 125, 673, 36, 673, 144, 673, 16, 673, 982, 673, 106, 673, 138, 673, 125, 673, 36, 673, 144, 673, 16, 673, 982, 673, 106, 673, 138, 673, 125, 673, 36, 673, 144, 673, 16, 673, 982, 673, 106, 673, 138, 673, 125, 673, 36, 673, 144, 673, 16, 673, 982, 673, 106, 673, 138, 3690, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seHGbBkVfqUM",
        "outputId": "bbbc6e9b-fed4-499e-840a-cb3783ce86f0"
      },
      "source": [
        "positive_skip_grams_list21"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [673, 125],\n",
              " [673, 36],\n",
              " [673, 144],\n",
              " [673, 16],\n",
              " [673, 982],\n",
              " [673, 106],\n",
              " [673, 138],\n",
              " [3690, 200],\n",
              " [3690, 64],\n",
              " [3690, 4],\n",
              " [3690, 344],\n",
              " [3690, 4],\n",
              " [3690, 200],\n",
              " [3690, 64],\n",
              " [3690, 4],\n",
              " [3690, 344],\n",
              " [3690, 4],\n",
              " [3690, 200],\n",
              " [3690, 64],\n",
              " [3690, 4],\n",
              " [3690, 344],\n",
              " [3690, 4],\n",
              " [3690, 200],\n",
              " [3690, 64],\n",
              " [3690, 4],\n",
              " [3690, 344],\n",
              " [3690, 4],\n",
              " [3690, 200],\n",
              " [3690, 64],\n",
              " [3690, 4],\n",
              " [3690, 344],\n",
              " [3690, 4],\n",
              " [1286, 1286],\n",
              " [1286, 1286],\n",
              " [1286, 1286],\n",
              " [1286, 1286],\n",
              " [1390, 56],\n",
              " [1390, 27],\n",
              " [1390, 506],\n",
              " [1390, 40],\n",
              " [1390, 3],\n",
              " [1390, 57],\n",
              " [1390, 24],\n",
              " [1390, 56],\n",
              " [1390, 27],\n",
              " [1390, 506],\n",
              " [1390, 40],\n",
              " [1390, 3],\n",
              " [1390, 57],\n",
              " [1390, 24],\n",
              " [1390, 56],\n",
              " [1390, 27],\n",
              " [1390, 506],\n",
              " [1390, 40],\n",
              " [1390, 3],\n",
              " [1390, 57],\n",
              " [1390, 24],\n",
              " [1390, 56],\n",
              " [1390, 27],\n",
              " [1390, 506],\n",
              " [1390, 40],\n",
              " [1390, 3],\n",
              " [1390, 57],\n",
              " [1390, 24],\n",
              " [1390, 56],\n",
              " [1390, 27],\n",
              " [1390, 506],\n",
              " [1390, 40],\n",
              " [1390, 3],\n",
              " [1390, 57],\n",
              " [1390, 24],\n",
              " [1390, 56],\n",
              " [1390, 27],\n",
              " [1390, 506],\n",
              " [1390, 40],\n",
              " [1390, 3],\n",
              " [1390, 57],\n",
              " [1390, 24],\n",
              " [1390, 56],\n",
              " [1390, 27],\n",
              " [1390, 506],\n",
              " [1390, 40],\n",
              " [1390, 3],\n",
              " [1390, 57],\n",
              " [1390, 24],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [2863, 885],\n",
              " [2863, 18],\n",
              " [2863, 54],\n",
              " [2863, 72],\n",
              " [54, 18],\n",
              " [146, 18],\n",
              " [54, 2863],\n",
              " [146, 146],\n",
              " [2863, 17],\n",
              " [54, 72],\n",
              " [2863, 163],\n",
              " [146, 17],\n",
              " [54, 885],\n",
              " [54, 17],\n",
              " [54, 32],\n",
              " [146, 72],\n",
              " [146, 885],\n",
              " [2863, 32],\n",
              " [146, 163],\n",
              " [1323, 79],\n",
              " [1323, 47],\n",
              " [1323, 1],\n",
              " [1323, 29],\n",
              " [1323, 1],\n",
              " [1323, 58],\n",
              " [1323, 79],\n",
              " [1323, 47],\n",
              " [1323, 1],\n",
              " [1323, 29],\n",
              " [1323, 1],\n",
              " [1323, 58],\n",
              " [1323, 79],\n",
              " [1323, 47],\n",
              " [1323, 1],\n",
              " [1323, 29],\n",
              " [1323, 1],\n",
              " [1323, 58],\n",
              " [1323, 79],\n",
              " [1323, 47],\n",
              " [1323, 1],\n",
              " [1323, 29],\n",
              " [1323, 1],\n",
              " [1323, 58],\n",
              " [1323, 79],\n",
              " [1323, 47],\n",
              " [1323, 1],\n",
              " [1323, 29],\n",
              " [1323, 1],\n",
              " [1323, 58],\n",
              " [1323, 79],\n",
              " [1323, 47],\n",
              " [1323, 1],\n",
              " [1323, 29],\n",
              " [1323, 1],\n",
              " [1323, 58],\n",
              " [573, 79],\n",
              " [573, 22],\n",
              " [573, 1],\n",
              " [573, 58],\n",
              " [573, 2],\n",
              " [573, 334],\n",
              " [573, 79],\n",
              " [573, 22],\n",
              " [573, 1],\n",
              " [573, 58],\n",
              " [573, 2],\n",
              " [573, 334],\n",
              " [573, 79],\n",
              " [573, 22],\n",
              " [573, 1],\n",
              " [573, 58],\n",
              " [573, 2],\n",
              " [573, 334],\n",
              " [573, 79],\n",
              " [573, 22],\n",
              " [573, 1],\n",
              " [573, 58],\n",
              " [573, 2],\n",
              " [573, 334],\n",
              " [573, 79],\n",
              " [573, 22],\n",
              " [573, 1],\n",
              " [573, 58],\n",
              " [573, 2],\n",
              " [573, 334],\n",
              " [573, 79],\n",
              " [573, 22],\n",
              " [573, 1],\n",
              " [573, 58],\n",
              " [573, 2],\n",
              " [573, 334],\n",
              " [2871, 66],\n",
              " [2871, 12],\n",
              " [2871, 4],\n",
              " [2871, 9],\n",
              " [2871, 1375],\n",
              " [2871, 66],\n",
              " [2871, 12],\n",
              " [2871, 4],\n",
              " [2871, 9],\n",
              " [2871, 1375],\n",
              " [2871, 66],\n",
              " [2871, 12],\n",
              " [2871, 4],\n",
              " [2871, 9],\n",
              " [2871, 1375],\n",
              " [2871, 66],\n",
              " [2871, 12],\n",
              " [2871, 4],\n",
              " [2871, 9],\n",
              " [2871, 1375],\n",
              " [2871, 66],\n",
              " [2871, 12],\n",
              " [2871, 4],\n",
              " [2871, 9],\n",
              " [2871, 1375],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [3329, 14],\n",
              " [3329, 14],\n",
              " [3329, 2461],\n",
              " [3329, 1],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [3329, 14],\n",
              " [3329, 14],\n",
              " [3329, 2461],\n",
              " [3329, 1],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [3329, 14],\n",
              " [3329, 14],\n",
              " [3329, 2461],\n",
              " [3329, 1],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [3329, 14],\n",
              " [3329, 14],\n",
              " [3329, 2461],\n",
              " [3329, 1],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [3329, 14],\n",
              " [3329, 14],\n",
              " [3329, 2461],\n",
              " [3329, 1],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [3329, 14],\n",
              " [3329, 14],\n",
              " [3329, 2461],\n",
              " [3329, 1],\n",
              " [270, 165],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " [9, 157],\n",
              " [339, 2],\n",
              " [339, 4],\n",
              " [9, 27],\n",
              " [339, 27],\n",
              " [339, 1033],\n",
              " [9, 339],\n",
              " [1033, 157],\n",
              " [9, 2],\n",
              " [1033, 2],\n",
              " [9, 89],\n",
              " [339, 89],\n",
              " [9, 170],\n",
              " [1033, 1],\n",
              " [1033, 4],\n",
              " [9, 1],\n",
              " [9, 4],\n",
              " [339, 170],\n",
              " [1033, 27],\n",
              " [1033, 339],\n",
              " [9, 1033],\n",
              " [1033, 89],\n",
              " [339, 157],\n",
              " [339, 9],\n",
              " [1033, 9],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPWyZDodyQUa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPGjuXRPyQXN",
        "outputId": "248a9137-4ac4-4e99-985b-c21f76e10517"
      },
      "source": [
        "targets11, contexts11, labels11,positive_skip_grams_list11 = generate_training_data1(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets11), len(contexts11), len(labels11))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 32777/32777 [02:07<00:00, 256.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "65715 65715 65715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cDAdg1byQZ3",
        "outputId": "ba8e861c-a098-450f-a5ac-4f404953e937"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset11 = tf.data.Dataset.from_tensor_slices(((targets11, contexts11), labels11))\n",
        "dataset11 = dataset11.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset11)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMhPSgpSyQcO",
        "outputId": "1c515ba7-d63d-4323-b8f6-e1150a5170b2"
      },
      "source": [
        "dataset11 = dataset11.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Pqq6ZYyQe5"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec11 = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec11.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPy9dRfty5st",
        "outputId": "42423940-35ec-49c1-84df-752386864025"
      },
      "source": [
        "word2vec11.fit(dataset11, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "64/64 [==============================] - 2s 22ms/step - loss: 1.6083 - accuracy: 0.2328\n",
            "Epoch 2/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 1.5892 - accuracy: 0.5542\n",
            "Epoch 3/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 1.5413 - accuracy: 0.6065\n",
            "Epoch 4/20\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 1.4569 - accuracy: 0.5775\n",
            "Epoch 5/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 1.3571 - accuracy: 0.5826\n",
            "Epoch 6/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 1.2595 - accuracy: 0.6090\n",
            "Epoch 7/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 1.1687 - accuracy: 0.6436\n",
            "Epoch 8/20\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 1.0846 - accuracy: 0.6785\n",
            "Epoch 9/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 1.0066 - accuracy: 0.7103\n",
            "Epoch 10/20\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.9344 - accuracy: 0.7399\n",
            "Epoch 11/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.8676 - accuracy: 0.7651\n",
            "Epoch 12/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.8060 - accuracy: 0.7860\n",
            "Epoch 13/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.7493 - accuracy: 0.8055\n",
            "Epoch 14/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.6973 - accuracy: 0.8228\n",
            "Epoch 15/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.6498 - accuracy: 0.8381\n",
            "Epoch 16/20\n",
            "64/64 [==============================] - 1s 17ms/step - loss: 0.6063 - accuracy: 0.8527\n",
            "Epoch 17/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.5667 - accuracy: 0.8652\n",
            "Epoch 18/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.5307 - accuracy: 0.8759\n",
            "Epoch 19/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.4978 - accuracy: 0.8856\n",
            "Epoch 20/20\n",
            "64/64 [==============================] - 1s 18ms/step - loss: 0.4679 - accuracy: 0.8943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1bf4697910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "03qUTaTOy52c",
        "outputId": "5e026ed5-199a-42e3-81c2-96b08208ebba"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1256), started 0:25:40 ago. (Use '!kill 1256' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKUkpPRrzrgT",
        "outputId": "5922c457-6487-4ceb-9abc-9d046601d50a"
      },
      "source": [
        "positive_skip_grams_list11.count('1504')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGp_wvGCy57H",
        "outputId": "ce26557a-1da4-4406-8d9b-e546b2c403e0"
      },
      "source": [
        "for target111, context111 in positive_skip_grams_list11[:1000]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target111]}, {inverse_vocab[context111]})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, [3 2 1 4 3]): (proceed, any)\n",
            "(4, [3 2 1 4 3]): (proceed, we)\n",
            "(4, [3 2 1 4 3]): (proceed, before)\n",
            "(4, [3 2 1 4 3]): (proceed, further)\n",
            "(4, [3 2 1 4 3]): (proceed, any)\n",
            "(4, [3 2 1 4 3]): (proceed, we)\n",
            "(4, [3 2 1 4 3]): (proceed, before)\n",
            "(4, [3 2 1 4 3]): (proceed, further)\n",
            "(4, [3 2 1 4 3]): (proceed, any)\n",
            "(4, [3 2 1 4 3]): (proceed, we)\n",
            "(4, [3 2 1 4 3]): (proceed, before)\n",
            "(4, [3 2 1 4 3]): (proceed, further)\n",
            "(4, [3 2 1 4 3]): (proceed, any)\n",
            "(4, [3 2 1 4 3]): (proceed, we)\n",
            "(4, [3 2 1 4 3]): (proceed, before)\n",
            "(4, [3 2 1 4 3]): (proceed, further)\n",
            "(4, [3 2 1 4 3]): (famish, than)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (famish, than)\n",
            "(4, [3 2 1 4 3]): (famish, to)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (resolved, resolved)\n",
            "(4, [3 2 1 4 3]): (we, we)\n",
            "(4, [3 2 1 4 3]): (we, knowt)\n",
            "(4, [3 2 1 4 3]): (we, we)\n",
            "(4, [3 2 1 4 3]): (we, knowt)\n",
            "(4, [3 2 1 4 3]): (kill, us)\n",
            "(4, [3 2 1 4 3]): (kill, let)\n",
            "(4, [3 2 1 4 3]): (kill, and)\n",
            "(4, [3 2 1 4 3]): (kill, him)\n",
            "(4, [3 2 1 4 3]): (kill, us)\n",
            "(4, [3 2 1 4 3]): (kill, let)\n",
            "(4, [3 2 1 4 3]): (kill, and)\n",
            "(4, [3 2 1 4 3]): (kill, him)\n",
            "(4, [3 2 1 4 3]): (kill, us)\n",
            "(4, [3 2 1 4 3]): (kill, let)\n",
            "(4, [3 2 1 4 3]): (kill, and)\n",
            "(4, [3 2 1 4 3]): (kill, him)\n",
            "(4, [3 2 1 4 3]): (kill, us)\n",
            "(4, [3 2 1 4 3]): (kill, let)\n",
            "(4, [3 2 1 4 3]): (kill, and)\n",
            "(4, [3 2 1 4 3]): (kill, him)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (ont, more)\n",
            "(4, [3 2 1 4 3]): (talking, more)\n",
            "(4, [3 2 1 4 3]): (talking, no)\n",
            "(4, [3 2 1 4 3]): (talking, ont)\n",
            "(4, [3 2 1 4 3]): (talking, let)\n",
            "(4, [3 2 1 4 3]): (ont, talking)\n",
            "(4, [3 2 1 4 3]): (ont, let)\n",
            "(4, [3 2 1 4 3]): (ont, it)\n",
            "(4, [3 2 1 4 3]): (good, citizens)\n",
            "(4, [3 2 1 4 3]): (good, word)\n",
            "(4, [3 2 1 4 3]): (good, one)\n",
            "(4, [3 2 1 4 3]): (good, citizens)\n",
            "(4, [3 2 1 4 3]): (good, word)\n",
            "(4, [3 2 1 4 3]): (good, one)\n",
            "(4, [3 2 1 4 3]): (good, citizens)\n",
            "(4, [3 2 1 4 3]): (good, word)\n",
            "(4, [3 2 1 4 3]): (good, one)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (citizens, patricians)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (good, the)\n",
            "(4, [3 2 1 4 3]): (good, patricians)\n",
            "(4, [3 2 1 4 3]): (citizens, poor)\n",
            "(4, [3 2 1 4 3]): (patricians, citizens)\n",
            "(4, [3 2 1 4 3]): (patricians, good)\n",
            "(4, [3 2 1 4 3]): (citizens, [UNK])\n",
            "(4, [3 2 1 4 3]): (citizens, the)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (us, would)\n",
            "(4, [3 2 1 4 3]): (authority, [UNK])\n",
            "(4, [3 2 1 4 3]): (authority, on)\n",
            "(4, [3 2 1 4 3]): (us, [UNK])\n",
            "(4, [3 2 1 4 3]): (they, if)\n",
            "(4, [3 2 1 4 3]): (they, us)\n",
            "(4, [3 2 1 4 3]): (us, they)\n",
            "(4, [3 2 1 4 3]): (us, if)\n",
            "(4, [3 2 1 4 3]): (authority, what)\n",
            "(4, [3 2 1 4 3]): (wholesome, we)\n",
            "(4, [3 2 1 4 3]): (wholesome, might)\n",
            "(4, [3 2 1 4 3]): (wholesome, we)\n",
            "(4, [3 2 1 4 3]): (wholesome, might)\n",
            "(4, [3 2 1 4 3]): (misery, of)\n",
            "(4, [3 2 1 4 3]): (misery, is)\n",
            "(4, [3 2 1 4 3]): (misery, as)\n",
            "(4, [3 2 1 4 3]): (misery, our)\n",
            "(4, [3 2 1 4 3]): (misery, of)\n",
            "(4, [3 2 1 4 3]): (misery, is)\n",
            "(4, [3 2 1 4 3]): (misery, as)\n",
            "(4, [3 2 1 4 3]): (misery, our)\n",
            "(4, [3 2 1 4 3]): (misery, of)\n",
            "(4, [3 2 1 4 3]): (misery, is)\n",
            "(4, [3 2 1 4 3]): (misery, as)\n",
            "(4, [3 2 1 4 3]): (misery, our)\n",
            "(4, [3 2 1 4 3]): (misery, of)\n",
            "(4, [3 2 1 4 3]): (misery, is)\n",
            "(4, [3 2 1 4 3]): (misery, as)\n",
            "(4, [3 2 1 4 3]): (misery, our)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (revenge, let)\n",
            "(4, [3 2 1 4 3]): (revenge, this)\n",
            "(4, [3 2 1 4 3]): (revenge, us)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (revenge, let)\n",
            "(4, [3 2 1 4 3]): (revenge, this)\n",
            "(4, [3 2 1 4 3]): (revenge, us)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (revenge, let)\n",
            "(4, [3 2 1 4 3]): (revenge, this)\n",
            "(4, [3 2 1 4 3]): (revenge, us)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (revenge, let)\n",
            "(4, [3 2 1 4 3]): (revenge, this)\n",
            "(4, [3 2 1 4 3]): (revenge, us)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (sufferance, a)\n",
            "(4, [3 2 1 4 3]): (revenge, let)\n",
            "(4, [3 2 1 4 3]): (revenge, this)\n",
            "(4, [3 2 1 4 3]): (revenge, us)\n",
            "(4, [3 2 1 4 3]): (sufferance, is)\n",
            "(4, [3 2 1 4 3]): (gods, the)\n",
            "(4, [3 2 1 4 3]): (gods, for)\n",
            "(4, [3 2 1 4 3]): (gods, know)\n",
            "(4, [3 2 1 4 3]): (gods, the)\n",
            "(4, [3 2 1 4 3]): (gods, for)\n",
            "(4, [3 2 1 4 3]): (gods, know)\n",
            "(4, [3 2 1 4 3]): (gods, the)\n",
            "(4, [3 2 1 4 3]): (gods, for)\n",
            "(4, [3 2 1 4 3]): (gods, know)\n",
            "(4, [3 2 1 4 3]): (bread, in)\n",
            "(4, [3 2 1 4 3]): (bread, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (bread, not)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (bread, for)\n",
            "(4, [3 2 1 4 3]): (bread, in)\n",
            "(4, [3 2 1 4 3]): (bread, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (bread, not)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (bread, for)\n",
            "(4, [3 2 1 4 3]): (bread, in)\n",
            "(4, [3 2 1 4 3]): (bread, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (bread, not)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (bread, for)\n",
            "(4, [3 2 1 4 3]): (bread, in)\n",
            "(4, [3 2 1 4 3]): (bread, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (bread, not)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (bread, for)\n",
            "(4, [3 2 1 4 3]): (bread, in)\n",
            "(4, [3 2 1 4 3]): (bread, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (bread, not)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (bread, for)\n",
            "(4, [3 2 1 4 3]): (bread, in)\n",
            "(4, [3 2 1 4 3]): (bread, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (bread, not)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (bread, for)\n",
            "(4, [3 2 1 4 3]): (bread, in)\n",
            "(4, [3 2 1 4 3]): (bread, [UNK])\n",
            "(4, [3 2 1 4 3]): (thirst, for)\n",
            "(4, [3 2 1 4 3]): (bread, not)\n",
            "(4, [3 2 1 4 3]): (thirst, not)\n",
            "(4, [3 2 1 4 3]): (thirst, in)\n",
            "(4, [3 2 1 4 3]): (bread, for)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (marcius, against)\n",
            "(4, [3 2 1 4 3]): (proceed, would)\n",
            "(4, [3 2 1 4 3]): (marcius, caius)\n",
            "(4, [3 2 1 4 3]): (proceed, you)\n",
            "(4, [3 2 1 4 3]): (proceed, against)\n",
            "(4, [3 2 1 4 3]): (caius, against)\n",
            "(4, [3 2 1 4 3]): (caius, especially)\n",
            "(4, [3 2 1 4 3]): (caius, marcius)\n",
            "(4, [3 2 1 4 3]): (proceed, especially)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (dog, a)\n",
            "(4, [3 2 1 4 3]): (dog, to)\n",
            "(4, [3 2 1 4 3]): (dog, very)\n",
            "(4, [3 2 1 4 3]): (dog, the)\n",
            "(4, [3 2 1 4 3]): (services, has)\n",
            "(4, [3 2 1 4 3]): (services, he)\n",
            "(4, [3 2 1 4 3]): (services, you)\n",
            "(4, [3 2 1 4 3]): (services, what)\n",
            "(4, [3 2 1 4 3]): (services, has)\n",
            "(4, [3 2 1 4 3]): (services, he)\n",
            "(4, [3 2 1 4 3]): (services, you)\n",
            "(4, [3 2 1 4 3]): (services, what)\n",
            "(4, [3 2 1 4 3]): (services, has)\n",
            "(4, [3 2 1 4 3]): (services, he)\n",
            "(4, [3 2 1 4 3]): (services, you)\n",
            "(4, [3 2 1 4 3]): (services, what)\n",
            "(4, [3 2 1 4 3]): (services, has)\n",
            "(4, [3 2 1 4 3]): (services, he)\n",
            "(4, [3 2 1 4 3]): (services, you)\n",
            "(4, [3 2 1 4 3]): (services, what)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (pays, with)\n",
            "(4, [3 2 1 4 3]): (but, he)\n",
            "(4, [3 2 1 4 3]): (but, fort)\n",
            "(4, [3 2 1 4 3]): (pays, he)\n",
            "(4, [3 2 1 4 3]): (pays, that)\n",
            "(4, [3 2 1 4 3]): (pays, himself)\n",
            "(4, [3 2 1 4 3]): (but, report)\n",
            "(4, [3 2 1 4 3]): (but, that)\n",
            "(4, [3 2 1 4 3]): (what, you)\n",
            "(4, [3 2 1 4 3]): (what, he)\n",
            "(4, [3 2 1 4 3]): (what, hath)\n",
            "(4, [3 2 1 4 3]): (what, unto)\n",
            "(4, [3 2 1 4 3]): (what, you)\n",
            "(4, [3 2 1 4 3]): (what, he)\n",
            "(4, [3 2 1 4 3]): (what, hath)\n",
            "(4, [3 2 1 4 3]): (what, unto)\n",
            "(4, [3 2 1 4 3]): (what, you)\n",
            "(4, [3 2 1 4 3]): (what, he)\n",
            "(4, [3 2 1 4 3]): (what, hath)\n",
            "(4, [3 2 1 4 3]): (what, unto)\n",
            "(4, [3 2 1 4 3]): (what, you)\n",
            "(4, [3 2 1 4 3]): (what, he)\n",
            "(4, [3 2 1 4 3]): (what, hath)\n",
            "(4, [3 2 1 4 3]): (what, unto)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (partly, which)\n",
            "(4, [3 2 1 4 3]): (partly, be)\n",
            "(4, [3 2 1 4 3]): (mother, please)\n",
            "(4, [3 2 1 4 3]): (partly, proud)\n",
            "(4, [3 2 1 4 3]): (mother, and)\n",
            "(4, [3 2 1 4 3]): (partly, to)\n",
            "(4, [3 2 1 4 3]): (mother, to)\n",
            "(4, [3 2 1 4 3]): (mother, his)\n",
            "(4, [3 2 1 4 3]): (account, nature)\n",
            "(4, [3 2 1 4 3]): (account, you)\n",
            "(4, [3 2 1 4 3]): (account, a)\n",
            "(4, [3 2 1 4 3]): (cannot, in)\n",
            "(4, [3 2 1 4 3]): (cannot, help)\n",
            "(4, [3 2 1 4 3]): (cannot, what)\n",
            "(4, [3 2 1 4 3]): (cannot, he)\n",
            "(4, [3 2 1 4 3]): (account, nature)\n",
            "(4, [3 2 1 4 3]): (account, you)\n",
            "(4, [3 2 1 4 3]): (account, a)\n",
            "(4, [3 2 1 4 3]): (cannot, in)\n",
            "(4, [3 2 1 4 3]): (cannot, help)\n",
            "(4, [3 2 1 4 3]): (cannot, what)\n",
            "(4, [3 2 1 4 3]): (cannot, he)\n",
            "(4, [3 2 1 4 3]): (account, nature)\n",
            "(4, [3 2 1 4 3]): (account, you)\n",
            "(4, [3 2 1 4 3]): (account, a)\n",
            "(4, [3 2 1 4 3]): (cannot, in)\n",
            "(4, [3 2 1 4 3]): (cannot, help)\n",
            "(4, [3 2 1 4 3]): (cannot, what)\n",
            "(4, [3 2 1 4 3]): (cannot, he)\n",
            "(4, [3 2 1 4 3]): (account, nature)\n",
            "(4, [3 2 1 4 3]): (account, you)\n",
            "(4, [3 2 1 4 3]): (account, a)\n",
            "(4, [3 2 1 4 3]): (cannot, in)\n",
            "(4, [3 2 1 4 3]): (cannot, help)\n",
            "(4, [3 2 1 4 3]): (cannot, what)\n",
            "(4, [3 2 1 4 3]): (cannot, he)\n",
            "(4, [3 2 1 4 3]): (account, nature)\n",
            "(4, [3 2 1 4 3]): (account, you)\n",
            "(4, [3 2 1 4 3]): (account, a)\n",
            "(4, [3 2 1 4 3]): (cannot, in)\n",
            "(4, [3 2 1 4 3]): (cannot, help)\n",
            "(4, [3 2 1 4 3]): (cannot, what)\n",
            "(4, [3 2 1 4 3]): (cannot, he)\n",
            "(4, [3 2 1 4 3]): (account, nature)\n",
            "(4, [3 2 1 4 3]): (account, you)\n",
            "(4, [3 2 1 4 3]): (account, a)\n",
            "(4, [3 2 1 4 3]): (cannot, in)\n",
            "(4, [3 2 1 4 3]): (cannot, help)\n",
            "(4, [3 2 1 4 3]): (cannot, what)\n",
            "(4, [3 2 1 4 3]): (cannot, he)\n",
            "(4, [3 2 1 4 3]): (account, nature)\n",
            "(4, [3 2 1 4 3]): (account, you)\n",
            "(4, [3 2 1 4 3]): (account, a)\n",
            "(4, [3 2 1 4 3]): (cannot, in)\n",
            "(4, [3 2 1 4 3]): (cannot, help)\n",
            "(4, [3 2 1 4 3]): (cannot, what)\n",
            "(4, [3 2 1 4 3]): (cannot, he)\n",
            "(4, [3 2 1 4 3]): (need, be)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (need, i)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (need, be)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (need, i)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (need, be)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (need, i)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (need, be)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (need, i)\n",
            "(4, [3 2 1 4 3]): (need, not)\n",
            "(4, [3 2 1 4 3]): (shouts, these)\n",
            "(4, [3 2 1 4 3]): (shouts, are)\n",
            "(4, [3 2 1 4 3]): (shouts, what)\n",
            "(4, [3 2 1 4 3]): (shouts, these)\n",
            "(4, [3 2 1 4 3]): (shouts, are)\n",
            "(4, [3 2 1 4 3]): (shouts, what)\n",
            "(4, [3 2 1 4 3]): (shouts, these)\n",
            "(4, [3 2 1 4 3]): (shouts, are)\n",
            "(4, [3 2 1 4 3]): (shouts, what)\n",
            "(4, [3 2 1 4 3]): (citizen, first)\n",
            "(4, [3 2 1 4 3]): (honest, one)\n",
            "(4, [3 2 1 4 3]): (honest, would)\n",
            "(4, [3 2 1 4 3]): (honest, hes)\n",
            "(4, [3 2 1 4 3]): (honest, enough)\n",
            "(4, [3 2 1 4 3]): (honest, one)\n",
            "(4, [3 2 1 4 3]): (honest, would)\n",
            "(4, [3 2 1 4 3]): (honest, hes)\n",
            "(4, [3 2 1 4 3]): (honest, enough)\n",
            "(4, [3 2 1 4 3]): (honest, one)\n",
            "(4, [3 2 1 4 3]): (honest, would)\n",
            "(4, [3 2 1 4 3]): (honest, hes)\n",
            "(4, [3 2 1 4 3]): (honest, enough)\n",
            "(4, [3 2 1 4 3]): (honest, one)\n",
            "(4, [3 2 1 4 3]): (honest, would)\n",
            "(4, [3 2 1 4 3]): (honest, hes)\n",
            "(4, [3 2 1 4 3]): (honest, enough)\n",
            "(4, [3 2 1 4 3]): (hand, go)\n",
            "(4, [3 2 1 4 3]): (hand, where)\n",
            "(4, [3 2 1 4 3]): (hand, in)\n",
            "(4, [3 2 1 4 3]): (hand, countrymen)\n",
            "(4, [3 2 1 4 3]): (hand, go)\n",
            "(4, [3 2 1 4 3]): (hand, where)\n",
            "(4, [3 2 1 4 3]): (hand, in)\n",
            "(4, [3 2 1 4 3]): (hand, countrymen)\n",
            "(4, [3 2 1 4 3]): (hand, go)\n",
            "(4, [3 2 1 4 3]): (hand, where)\n",
            "(4, [3 2 1 4 3]): (hand, in)\n",
            "(4, [3 2 1 4 3]): (hand, countrymen)\n",
            "(4, [3 2 1 4 3]): (hand, go)\n",
            "(4, [3 2 1 4 3]): (hand, where)\n",
            "(4, [3 2 1 4 3]): (hand, in)\n",
            "(4, [3 2 1 4 3]): (hand, countrymen)\n",
            "(4, [3 2 1 4 3]): (speak, matter)\n",
            "(4, [3 2 1 4 3]): (speak, pray)\n",
            "(4, [3 2 1 4 3]): (speak, i)\n",
            "(4, [3 2 1 4 3]): (speak, the)\n",
            "(4, [3 2 1 4 3]): (speak, matter)\n",
            "(4, [3 2 1 4 3]): (speak, pray)\n",
            "(4, [3 2 1 4 3]): (speak, i)\n",
            "(4, [3 2 1 4 3]): (speak, the)\n",
            "(4, [3 2 1 4 3]): (speak, matter)\n",
            "(4, [3 2 1 4 3]): (speak, pray)\n",
            "(4, [3 2 1 4 3]): (speak, i)\n",
            "(4, [3 2 1 4 3]): (speak, the)\n",
            "(4, [3 2 1 4 3]): (speak, matter)\n",
            "(4, [3 2 1 4 3]): (speak, pray)\n",
            "(4, [3 2 1 4 3]): (speak, i)\n",
            "(4, [3 2 1 4 3]): (speak, the)\n",
            "(4, [3 2 1 4 3]): (senate, have)\n",
            "(4, [3 2 1 4 3]): (senate, the)\n",
            "(4, [3 2 1 4 3]): (senate, to)\n",
            "(4, [3 2 1 4 3]): (senate, they)\n",
            "(4, [3 2 1 4 3]): (senate, have)\n",
            "(4, [3 2 1 4 3]): (senate, the)\n",
            "(4, [3 2 1 4 3]): (senate, to)\n",
            "(4, [3 2 1 4 3]): (senate, they)\n",
            "(4, [3 2 1 4 3]): (senate, have)\n",
            "(4, [3 2 1 4 3]): (senate, the)\n",
            "(4, [3 2 1 4 3]): (senate, to)\n",
            "(4, [3 2 1 4 3]): (senate, they)\n",
            "(4, [3 2 1 4 3]): (senate, have)\n",
            "(4, [3 2 1 4 3]): (senate, the)\n",
            "(4, [3 2 1 4 3]): (senate, to)\n",
            "(4, [3 2 1 4 3]): (senate, they)\n",
            "(4, [3 2 1 4 3]): (deeds, say)\n",
            "(4, [3 2 1 4 3]): (deeds, em)\n",
            "(4, [3 2 1 4 3]): (deeds, in)\n",
            "(4, [3 2 1 4 3]): (deeds, they)\n",
            "(4, [3 2 1 4 3]): (deeds, say)\n",
            "(4, [3 2 1 4 3]): (deeds, em)\n",
            "(4, [3 2 1 4 3]): (deeds, in)\n",
            "(4, [3 2 1 4 3]): (deeds, they)\n",
            "(4, [3 2 1 4 3]): (deeds, say)\n",
            "(4, [3 2 1 4 3]): (deeds, em)\n",
            "(4, [3 2 1 4 3]): (deeds, in)\n",
            "(4, [3 2 1 4 3]): (deeds, they)\n",
            "(4, [3 2 1 4 3]): (deeds, say)\n",
            "(4, [3 2 1 4 3]): (deeds, em)\n",
            "(4, [3 2 1 4 3]): (deeds, in)\n",
            "(4, [3 2 1 4 3]): (deeds, they)\n",
            "(4, [3 2 1 4 3]): (too, strong)\n",
            "(4, [3 2 1 4 3]): (strong, too)\n",
            "(4, [3 2 1 4 3]): (strong, arms)\n",
            "(4, [3 2 1 4 3]): (too, arms)\n",
            "(4, [3 2 1 4 3]): (strong, have)\n",
            "(4, [3 2 1 4 3]): (too, strong)\n",
            "(4, [3 2 1 4 3]): (strong, too)\n",
            "(4, [3 2 1 4 3]): (strong, arms)\n",
            "(4, [3 2 1 4 3]): (too, arms)\n",
            "(4, [3 2 1 4 3]): (strong, have)\n",
            "(4, [3 2 1 4 3]): (too, strong)\n",
            "(4, [3 2 1 4 3]): (strong, too)\n",
            "(4, [3 2 1 4 3]): (strong, arms)\n",
            "(4, [3 2 1 4 3]): (too, arms)\n",
            "(4, [3 2 1 4 3]): (strong, have)\n",
            "(4, [3 2 1 4 3]): (too, strong)\n",
            "(4, [3 2 1 4 3]): (strong, too)\n",
            "(4, [3 2 1 4 3]): (strong, arms)\n",
            "(4, [3 2 1 4 3]): (too, arms)\n",
            "(4, [3 2 1 4 3]): (strong, have)\n",
            "(4, [3 2 1 4 3]): (too, strong)\n",
            "(4, [3 2 1 4 3]): (strong, too)\n",
            "(4, [3 2 1 4 3]): (strong, arms)\n",
            "(4, [3 2 1 4 3]): (too, arms)\n",
            "(4, [3 2 1 4 3]): (strong, have)\n",
            "(4, [3 2 1 4 3]): (neighbours, honest)\n",
            "(4, [3 2 1 4 3]): (masters, my)\n",
            "(4, [3 2 1 4 3]): (masters, good)\n",
            "(4, [3 2 1 4 3]): (masters, why)\n",
            "(4, [3 2 1 4 3]): (neighbours, mine)\n",
            "(4, [3 2 1 4 3]): (neighbours, honest)\n",
            "(4, [3 2 1 4 3]): (masters, my)\n",
            "(4, [3 2 1 4 3]): (masters, good)\n",
            "(4, [3 2 1 4 3]): (masters, why)\n",
            "(4, [3 2 1 4 3]): (neighbours, mine)\n",
            "(4, [3 2 1 4 3]): (neighbours, honest)\n",
            "(4, [3 2 1 4 3]): (masters, my)\n",
            "(4, [3 2 1 4 3]): (masters, good)\n",
            "(4, [3 2 1 4 3]): (masters, why)\n",
            "(4, [3 2 1 4 3]): (neighbours, mine)\n",
            "(4, [3 2 1 4 3]): (neighbours, honest)\n",
            "(4, [3 2 1 4 3]): (masters, my)\n",
            "(4, [3 2 1 4 3]): (masters, good)\n",
            "(4, [3 2 1 4 3]): (masters, why)\n",
            "(4, [3 2 1 4 3]): (neighbours, mine)\n",
            "(4, [3 2 1 4 3]): (neighbours, honest)\n",
            "(4, [3 2 1 4 3]): (masters, my)\n",
            "(4, [3 2 1 4 3]): (masters, good)\n",
            "(4, [3 2 1 4 3]): (masters, why)\n",
            "(4, [3 2 1 4 3]): (neighbours, mine)\n",
            "(4, [3 2 1 4 3]): (undo, you)\n",
            "(4, [3 2 1 4 3]): (undo, yourselves)\n",
            "(4, [3 2 1 4 3]): (undo, will)\n",
            "(4, [3 2 1 4 3]): (undo, you)\n",
            "(4, [3 2 1 4 3]): (undo, yourselves)\n",
            "(4, [3 2 1 4 3]): (undo, will)\n",
            "(4, [3 2 1 4 3]): (undo, you)\n",
            "(4, [3 2 1 4 3]): (undo, yourselves)\n",
            "(4, [3 2 1 4 3]): (undo, will)\n",
            "(4, [3 2 1 4 3]): (undone, we)\n",
            "(4, [3 2 1 4 3]): (undone, are)\n",
            "(4, [3 2 1 4 3]): (undone, already)\n",
            "(4, [3 2 1 4 3]): (undone, we)\n",
            "(4, [3 2 1 4 3]): (undone, are)\n",
            "(4, [3 2 1 4 3]): (undone, already)\n",
            "(4, [3 2 1 4 3]): (undone, we)\n",
            "(4, [3 2 1 4 3]): (undone, are)\n",
            "(4, [3 2 1 4 3]): (undone, already)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, you)\n",
            "(4, [3 2 1 4 3]): (patricians, of)\n",
            "(4, [3 2 1 4 3]): (patricians, have)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, you)\n",
            "(4, [3 2 1 4 3]): (patricians, of)\n",
            "(4, [3 2 1 4 3]): (patricians, have)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, you)\n",
            "(4, [3 2 1 4 3]): (patricians, of)\n",
            "(4, [3 2 1 4 3]): (patricians, have)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, you)\n",
            "(4, [3 2 1 4 3]): (patricians, of)\n",
            "(4, [3 2 1 4 3]): (patricians, have)\n",
            "(4, [3 2 1 4 3]): (strike, the)\n",
            "(4, [3 2 1 4 3]): (strike, at)\n",
            "(4, [3 2 1 4 3]): (strike, the)\n",
            "(4, [3 2 1 4 3]): (strike, at)\n",
            "(4, [3 2 1 4 3]): (roman, against)\n",
            "(4, [3 2 1 4 3]): (roman, whose)\n",
            "(4, [3 2 1 4 3]): (roman, the)\n",
            "(4, [3 2 1 4 3]): (roman, state)\n",
            "(4, [3 2 1 4 3]): (roman, against)\n",
            "(4, [3 2 1 4 3]): (roman, whose)\n",
            "(4, [3 2 1 4 3]): (roman, the)\n",
            "(4, [3 2 1 4 3]): (roman, state)\n",
            "(4, [3 2 1 4 3]): (roman, against)\n",
            "(4, [3 2 1 4 3]): (roman, whose)\n",
            "(4, [3 2 1 4 3]): (roman, the)\n",
            "(4, [3 2 1 4 3]): (roman, state)\n",
            "(4, [3 2 1 4 3]): (roman, against)\n",
            "(4, [3 2 1 4 3]): (roman, whose)\n",
            "(4, [3 2 1 4 3]): (roman, the)\n",
            "(4, [3 2 1 4 3]): (roman, state)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (cracking, takes)\n",
            "(4, [3 2 1 4 3]): (ten, cracking)\n",
            "(4, [3 2 1 4 3]): (ten, [UNK])\n",
            "(4, [3 2 1 4 3]): (cracking, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, ten)\n",
            "(4, [3 2 1 4 3]): (ten, thousand)\n",
            "(4, [3 2 1 4 3]): (cracking, it)\n",
            "(4, [3 2 1 4 3]): (ten, takes)\n",
            "(4, [3 2 1 4 3]): (strong, more)\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, of)\n",
            "(4, [3 2 1 4 3]): (strong, more)\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, of)\n",
            "(4, [3 2 1 4 3]): (strong, more)\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, of)\n",
            "(4, [3 2 1 4 3]): (strong, more)\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, [UNK])\n",
            "(4, [3 2 1 4 3]): (strong, of)\n",
            "(4, [3 2 1 4 3]): (dearth, the)\n",
            "(4, [3 2 1 4 3]): (dearth, for)\n",
            "(4, [3 2 1 4 3]): (dearth, the)\n",
            "(4, [3 2 1 4 3]): (dearth, for)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, not)\n",
            "(4, [3 2 1 4 3]): (patricians, make)\n",
            "(4, [3 2 1 4 3]): (patricians, it)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, not)\n",
            "(4, [3 2 1 4 3]): (patricians, make)\n",
            "(4, [3 2 1 4 3]): (patricians, it)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, not)\n",
            "(4, [3 2 1 4 3]): (patricians, make)\n",
            "(4, [3 2 1 4 3]): (patricians, it)\n",
            "(4, [3 2 1 4 3]): (patricians, the)\n",
            "(4, [3 2 1 4 3]): (patricians, not)\n",
            "(4, [3 2 1 4 3]): (patricians, make)\n",
            "(4, [3 2 1 4 3]): (patricians, it)\n",
            "(4, [3 2 1 4 3]): (alack, must)\n",
            "(4, [3 2 1 4 3]): (alack, help)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQo7fCMCy5-h",
        "outputId": "4ac6b6f5-0261-427b-9a31-a8bd15a4b70e"
      },
      "source": [
        "positive_skip_grams_list11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[982, 144],\n",
              " [982, 36],\n",
              " [982, 138],\n",
              " [982, 673],\n",
              " [982, 144],\n",
              " [982, 36],\n",
              " [982, 138],\n",
              " [982, 673],\n",
              " [982, 144],\n",
              " [982, 36],\n",
              " [982, 138],\n",
              " [982, 673],\n",
              " [982, 144],\n",
              " [982, 36],\n",
              " [982, 138],\n",
              " [982, 673],\n",
              " [3690, 64],\n",
              " [3690, 4],\n",
              " [3690, 64],\n",
              " [3690, 4],\n",
              " [1286, 1286],\n",
              " [1286, 1286],\n",
              " [1286, 1286],\n",
              " [1286, 1286],\n",
              " [36, 36],\n",
              " [36, 2655],\n",
              " [36, 36],\n",
              " [36, 2655],\n",
              " [506, 79],\n",
              " [506, 72],\n",
              " [506, 3],\n",
              " [506, 27],\n",
              " [506, 79],\n",
              " [506, 72],\n",
              " [506, 3],\n",
              " [506, 27],\n",
              " [506, 79],\n",
              " [506, 72],\n",
              " [506, 3],\n",
              " [506, 27],\n",
              " [506, 79],\n",
              " [506, 72],\n",
              " [506, 3],\n",
              " [506, 27],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [885, 54],\n",
              " [2863, 54],\n",
              " [2863, 32],\n",
              " [2863, 885],\n",
              " [2863, 72],\n",
              " [885, 2863],\n",
              " [885, 72],\n",
              " [885, 17],\n",
              " [46, 595],\n",
              " [46, 218],\n",
              " [46, 74],\n",
              " [46, 595],\n",
              " [46, 218],\n",
              " [46, 74],\n",
              " [46, 595],\n",
              " [46, 218],\n",
              " [46, 74],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [595, 1780],\n",
              " [1780, 2],\n",
              " [46, 2],\n",
              " [46, 1780],\n",
              " [595, 172],\n",
              " [1780, 595],\n",
              " [1780, 46],\n",
              " [595, 1],\n",
              " [595, 2],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [79, 58],\n",
              " [1323, 1],\n",
              " [1323, 47],\n",
              " [79, 1],\n",
              " [60, 39],\n",
              " [60, 79],\n",
              " [79, 60],\n",
              " [79, 39],\n",
              " [1323, 29],\n",
              " [1870, 36],\n",
              " [1870, 258],\n",
              " [1870, 36],\n",
              " [1870, 258],\n",
              " [1540, 6],\n",
              " [1540, 12],\n",
              " [1540, 25],\n",
              " [1540, 40],\n",
              " [1540, 6],\n",
              " [1540, 12],\n",
              " [1540, 25],\n",
              " [1540, 40],\n",
              " [1540, 6],\n",
              " [1540, 12],\n",
              " [1540, 25],\n",
              " [1540, 40],\n",
              " [1540, 6],\n",
              " [1540, 12],\n",
              " [1540, 25],\n",
              " [1540, 40],\n",
              " [2871, 9],\n",
              " [625, 72],\n",
              " [625, 21],\n",
              " [625, 79],\n",
              " [2871, 12],\n",
              " [2871, 9],\n",
              " [625, 72],\n",
              " [625, 21],\n",
              " [625, 79],\n",
              " [2871, 12],\n",
              " [2871, 9],\n",
              " [625, 72],\n",
              " [625, 21],\n",
              " [625, 79],\n",
              " [2871, 12],\n",
              " [2871, 9],\n",
              " [625, 72],\n",
              " [625, 21],\n",
              " [625, 79],\n",
              " [2871, 12],\n",
              " [2871, 9],\n",
              " [625, 72],\n",
              " [625, 21],\n",
              " [625, 79],\n",
              " [2871, 12],\n",
              " [260, 2],\n",
              " [260, 14],\n",
              " [260, 93],\n",
              " [260, 2],\n",
              " [260, 14],\n",
              " [260, 93],\n",
              " [260, 2],\n",
              " [260, 14],\n",
              " [260, 93],\n",
              " [2461, 11],\n",
              " [2461, 1],\n",
              " [3329, 14],\n",
              " [2461, 13],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [2461, 14],\n",
              " [2461, 11],\n",
              " [2461, 1],\n",
              " [3329, 14],\n",
              " [2461, 13],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [2461, 14],\n",
              " [2461, 11],\n",
              " [2461, 1],\n",
              " [3329, 14],\n",
              " [2461, 13],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [2461, 14],\n",
              " [2461, 11],\n",
              " [2461, 1],\n",
              " [3329, 14],\n",
              " [2461, 13],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [2461, 14],\n",
              " [2461, 11],\n",
              " [2461, 1],\n",
              " [3329, 14],\n",
              " [2461, 13],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [2461, 14],\n",
              " [2461, 11],\n",
              " [2461, 1],\n",
              " [3329, 14],\n",
              " [2461, 13],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [2461, 14],\n",
              " [2461, 11],\n",
              " [2461, 1],\n",
              " [3329, 14],\n",
              " [2461, 13],\n",
              " [3329, 13],\n",
              " [3329, 11],\n",
              " [2461, 14],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [225, 170],\n",
              " [982, 58],\n",
              " [225, 1187],\n",
              " [982, 7],\n",
              " [982, 170],\n",
              " [1187, 170],\n",
              " [1187, 3696],\n",
              " [1187, 225],\n",
              " [982, 3696],\n",
              " [1033, 9],\n",
              " [1033, 4],\n",
              " [1033, 157],\n",
              " [1033, 2],\n",
              " [1033, 9],\n",
              " [1033, 4],\n",
              " [1033, 157],\n",
              " [1033, 2],\n",
              " [1033, 9],\n",
              " [1033, 4],\n",
              " [1033, 157],\n",
              " [1033, 2],\n",
              " [1033, 9],\n",
              " [1033, 4],\n",
              " [1033, 157],\n",
              " [1033, 2],\n",
              " [1519, 320],\n",
              " [1519, 23],\n",
              " [1519, 7],\n",
              " [1519, 29],\n",
              " [1519, 320],\n",
              " [1519, 23],\n",
              " [1519, 7],\n",
              " [1519, 29],\n",
              " [1519, 320],\n",
              " [1519, 23],\n",
              " [1519, 7],\n",
              " [1519, 29],\n",
              " [1519, 320],\n",
              " [1519, 23],\n",
              " [1519, 7],\n",
              " [1519, 29],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [3500, 15],\n",
              " [22, 23],\n",
              " [22, 1169],\n",
              " [3500, 23],\n",
              " [3500, 10],\n",
              " [3500, 245],\n",
              " [22, 556],\n",
              " [22, 10],\n",
              " [29, 7],\n",
              " [29, 23],\n",
              " [29, 70],\n",
              " [29, 214],\n",
              " [29, 7],\n",
              " [29, 23],\n",
              " [29, 70],\n",
              " [29, 214],\n",
              " [29, 7],\n",
              " [29, 23],\n",
              " [29, 70],\n",
              " [29, 214],\n",
              " [29, 7],\n",
              " [29, 23],\n",
              " [29, 70],\n",
              " [29, 214],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1781, 53],\n",
              " [1781, 18],\n",
              " [223, 303],\n",
              " [1781, 450],\n",
              " [223, 3],\n",
              " [1781, 4],\n",
              " [223, 4],\n",
              " [223, 20],\n",
              " [1199, 408],\n",
              " [1199, 7],\n",
              " [1199, 9],\n",
              " [142, 11],\n",
              " [142, 309],\n",
              " [142, 29],\n",
              " [142, 23],\n",
              " [1199, 408],\n",
              " [1199, 7],\n",
              " [1199, 9],\n",
              " [142, 11],\n",
              " [142, 309],\n",
              " [142, 29],\n",
              " [142, 23],\n",
              " [1199, 408],\n",
              " [1199, 7],\n",
              " [1199, 9],\n",
              " [142, 11],\n",
              " [142, 309],\n",
              " [142, 29],\n",
              " [142, 23],\n",
              " [1199, 408],\n",
              " [1199, 7],\n",
              " [1199, 9],\n",
              " [142, 11],\n",
              " [142, 309],\n",
              " [142, 29],\n",
              " [142, 23],\n",
              " [1199, 408],\n",
              " [1199, 7],\n",
              " [1199, 9],\n",
              " [142, 11],\n",
              " [142, 309],\n",
              " [142, 29],\n",
              " [142, 23],\n",
              " [1199, 408],\n",
              " [1199, 7],\n",
              " [1199, 9],\n",
              " [142, 11],\n",
              " [142, 309],\n",
              " [142, 29],\n",
              " [142, 23],\n",
              " [1199, 408],\n",
              " [1199, 7],\n",
              " [1199, 9],\n",
              " [142, 11],\n",
              " [142, 309],\n",
              " [142, 29],\n",
              " [142, 23],\n",
              " [451, 18],\n",
              " [451, 13],\n",
              " [451, 5],\n",
              " [451, 13],\n",
              " [451, 18],\n",
              " [451, 13],\n",
              " [451, 5],\n",
              " [451, 13],\n",
              " [451, 18],\n",
              " [451, 13],\n",
              " [451, 5],\n",
              " [451, 13],\n",
              " [451, 18],\n",
              " [451, 13],\n",
              " [451, 5],\n",
              " [451, 13],\n",
              " [4083, 104],\n",
              " [4083, 41],\n",
              " [4083, 29],\n",
              " [4083, 104],\n",
              " [4083, 41],\n",
              " [4083, 29],\n",
              " [4083, 104],\n",
              " [4083, 41],\n",
              " [4083, 29],\n",
              " [270, 89],\n",
              " [605, 74],\n",
              " [605, 58],\n",
              " [605, 339],\n",
              " [605, 394],\n",
              " [605, 74],\n",
              " [605, 58],\n",
              " [605, 339],\n",
              " [605, 394],\n",
              " [605, 74],\n",
              " [605, 58],\n",
              " [605, 339],\n",
              " [605, 394],\n",
              " [605, 74],\n",
              " [605, 58],\n",
              " [605, 339],\n",
              " [605, 394],\n",
              " [158, 75],\n",
              " [158, 97],\n",
              " [158, 11],\n",
              " [158, 2208],\n",
              " [158, 75],\n",
              " [158, 97],\n",
              " [158, 11],\n",
              " [158, 2208],\n",
              " [158, 75],\n",
              " [158, 97],\n",
              " [158, 11],\n",
              " [158, 2208],\n",
              " [158, 75],\n",
              " [158, 97],\n",
              " [158, 11],\n",
              " [158, 2208],\n",
              " [106, 390],\n",
              " [106, 160],\n",
              " [106, 5],\n",
              " [106, 2],\n",
              " [106, 390],\n",
              " [106, 160],\n",
              " [106, 5],\n",
              " [106, 2],\n",
              " [106, 390],\n",
              " [106, 160],\n",
              " [106, 5],\n",
              " [106, 2],\n",
              " [106, 390],\n",
              " [106, 160],\n",
              " [106, 5],\n",
              " [106, 2],\n",
              " [1520, 24],\n",
              " [1520, 2],\n",
              " [1520, 4],\n",
              " [1520, 60],\n",
              " [1520, 24],\n",
              " [1520, 2],\n",
              " [1520, 4],\n",
              " [1520, 60],\n",
              " [1520, 24],\n",
              " [1520, 2],\n",
              " [1520, 4],\n",
              " [1520, 60],\n",
              " [1520, 24],\n",
              " [1520, 2],\n",
              " [1520, 4],\n",
              " [1520, 60],\n",
              " [780, 71],\n",
              " [780, 847],\n",
              " [780, 11],\n",
              " [780, 60],\n",
              " [780, 71],\n",
              " [780, 847],\n",
              " [780, 11],\n",
              " [780, 60],\n",
              " [780, 71],\n",
              " [780, 847],\n",
              " [780, 11],\n",
              " [780, 60],\n",
              " [780, 71],\n",
              " [780, 847],\n",
              " [780, 11],\n",
              " [780, 60],\n",
              " [100, 751],\n",
              " [751, 100],\n",
              " [751, 395],\n",
              " [100, 395],\n",
              " [751, 24],\n",
              " [100, 751],\n",
              " [751, 100],\n",
              " [751, 395],\n",
              " [100, 395],\n",
              " [751, 24],\n",
              " [100, 751],\n",
              " [751, 100],\n",
              " [751, 395],\n",
              " [100, 395],\n",
              " [751, 24],\n",
              " [100, 751],\n",
              " [751, 100],\n",
              " [751, 395],\n",
              " [100, 395],\n",
              " [751, 24],\n",
              " [100, 751],\n",
              " [751, 100],\n",
              " [751, 395],\n",
              " [100, 395],\n",
              " [751, 24],\n",
              " [1942, 605],\n",
              " [628, 8],\n",
              " [628, 46],\n",
              " [628, 90],\n",
              " [1942, 109],\n",
              " [1942, 605],\n",
              " [628, 8],\n",
              " [628, 46],\n",
              " [628, 90],\n",
              " [1942, 109],\n",
              " [1942, 605],\n",
              " [628, 8],\n",
              " [628, 46],\n",
              " [628, 90],\n",
              " [1942, 109],\n",
              " [1942, 605],\n",
              " [628, 8],\n",
              " [628, 46],\n",
              " [628, 90],\n",
              " [1942, 109],\n",
              " [1942, 605],\n",
              " [628, 8],\n",
              " [628, 46],\n",
              " [628, 90],\n",
              " [1942, 109],\n",
              " [2840, 7],\n",
              " [2840, 969],\n",
              " [2840, 31],\n",
              " [2840, 7],\n",
              " [2840, 969],\n",
              " [2840, 31],\n",
              " [2840, 7],\n",
              " [2840, 969],\n",
              " [2840, 31],\n",
              " [1268, 36],\n",
              " [1268, 41],\n",
              " [1268, 702],\n",
              " [1268, 36],\n",
              " [1268, 41],\n",
              " [1268, 702],\n",
              " [1268, 36],\n",
              " [1268, 41],\n",
              " [1268, 702],\n",
              " [1780, 2],\n",
              " [1780, 7],\n",
              " [1780, 6],\n",
              " [1780, 24],\n",
              " [1780, 2],\n",
              " [1780, 7],\n",
              " [1780, 6],\n",
              " [1780, 24],\n",
              " [1780, 2],\n",
              " [1780, 7],\n",
              " [1780, 6],\n",
              " [1780, 24],\n",
              " [1780, 2],\n",
              " [1780, 7],\n",
              " [1780, 6],\n",
              " [1780, 24],\n",
              " [537, 2],\n",
              " [537, 57],\n",
              " [537, 2],\n",
              " [537, 57],\n",
              " [820, 170],\n",
              " [820, 202],\n",
              " [820, 2],\n",
              " [820, 329],\n",
              " [820, 170],\n",
              " [820, 202],\n",
              " [820, 2],\n",
              " [820, 329],\n",
              " [820, 170],\n",
              " [820, 202],\n",
              " [820, 2],\n",
              " [820, 329],\n",
              " [820, 170],\n",
              " [820, 202],\n",
              " [820, 2],\n",
              " [820, 329],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [3150, 1623],\n",
              " [610, 3150],\n",
              " [610, 1],\n",
              " [3150, 352],\n",
              " [3150, 610],\n",
              " [610, 352],\n",
              " [3150, 17],\n",
              " [610, 1623],\n",
              " [751, 54],\n",
              " [751, 1],\n",
              " [751, 1],\n",
              " [751, 6],\n",
              " [751, 54],\n",
              " [751, 1],\n",
              " [751, 1],\n",
              " [751, 6],\n",
              " [751, 54],\n",
              " [751, 1],\n",
              " [751, 1],\n",
              " [751, 6],\n",
              " [751, 54],\n",
              " [751, 1],\n",
              " [751, 1],\n",
              " [751, 6],\n",
              " [3136, 2],\n",
              " [3136, 14],\n",
              " [3136, 2],\n",
              " [3136, 14],\n",
              " [1780, 2],\n",
              " [1780, 13],\n",
              " [1780, 80],\n",
              " [1780, 17],\n",
              " [1780, 2],\n",
              " [1780, 13],\n",
              " [1780, 80],\n",
              " [1780, 17],\n",
              " [1780, 2],\n",
              " [1780, 13],\n",
              " [1780, 80],\n",
              " [1780, 17],\n",
              " [1780, 2],\n",
              " [1780, 13],\n",
              " [1780, 80],\n",
              " [1780, 17],\n",
              " [833, 86],\n",
              " [833, 309],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOwUZCyt8_BV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHY9x14AISKP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxVt1TcHISMi"
      },
      "source": [
        "weights2 = word2vec2.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab2 = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBLzt8KaISPK"
      },
      "source": [
        "out_v = io.open('vectors_5.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata_5.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights2[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-T3for_DIgwT",
        "outputId": "e6b38aab-d890-4950-e3a1-1b19c62bf5fe"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors_5.tsv')\n",
        "  files.download('metadata_5.tsv')\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1dd977a6-b162-4bb4-8dda-fed67710adae\", \"vectors_5.tsv\", 6076026)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3a068973-bcf0-42b0-b70f-ef452d33aa1b\", \"metadata_5.tsv\", 28737)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjuakbUPIg7v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytib05rbnqeK"
      },
      "source": [
        "vocab_size1 = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRl0HEf1nqgz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJOzgWbQnqki"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}