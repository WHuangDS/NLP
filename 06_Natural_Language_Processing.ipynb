{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwsPppAAl6h3"
      },
      "source": [
        "# SCS 3546 Week 6 - Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZA7DZwpl6h9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n33R4EgHl6h-"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nn4qFhfl6h-"
      },
      "source": [
        "- Develop some familiarity with key concepts in NLP\n",
        "- Understanding Regular Expressions, n-grams, annotators, word and document representation, language and topic modeling\n",
        "- Have a look at some of the features of the Natural Language Toolkit (NLTK), spaCy, gensim, coreNLP and keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ENNGOpl6h_"
      },
      "source": [
        "## What is Natural Language Processing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6egIKpAcl6h_"
      },
      "source": [
        "- Natural Language Processing (NLP) is the study of computational treatment of natural (human) language\n",
        "\n",
        "- NLP applications include:\n",
        "  - Search: Web, documents, autocomplete\n",
        "  - Editing: Spelling, grammar, style\n",
        "  - Dialog: Chatbots, assistants\n",
        "  - Writing: Index, concordance, table of contents\n",
        "  - Email: Spam filter, classification, prioritization\n",
        "  - Text mining: Summarization, knowledge extraction, medical diagnosis\n",
        "  - Law: Legal inference, precedent search, subpoena classification\n",
        "  - News: Event detection, fact checking, headline composition\n",
        "  - Attribution: Plagiarism detection, literary forensics, style coaching\n",
        "  - Sentiment analysis: Community morale monitoring, product review triage, customer care\n",
        "  - Behavior prediction: Finance, election forecasting, marketing\n",
        "  - Creative writing: Movie scripts, poetry, song lyrics\n",
        "  \n",
        "Source: Natural Language Processing in Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36gh0VYll6iA"
      },
      "source": [
        "## Ambiguity in Natural Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvGB4n2El6iA"
      },
      "source": [
        "Ambiguity in language is one of the main issues in interpreting the concepts they represent. Consider the following sentences:\n",
        "\n",
        "- \"Teachers Strikes Idle Kids.\"\n",
        "- “Eats shoots and leaves”\n",
        "- \"Stolen Painting Found by Tree.\"\n",
        "- \"Local High School Dropouts Cut in Half\"\n",
        "\n",
        "\n",
        "Grammar is a description, not a prescription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdfLYcLll6iB"
      },
      "source": [
        "## Other Issues\n",
        "- Synonyms\n",
        "       A synonym is a word or phrase that means exactly or nearly the same as another lexeme in the same language.\n",
        "- Homonyms\n",
        "        In linguistics, homonyms, broadly defined, are words which sound alike or are spelled alike, but have different meanings.\n",
        "- Misspellings\n",
        "        an incorrect spelling\n",
        "- Sarcasm\n",
        "        the use of irony to mock or convey contempt.\n",
        "- Allegory\n",
        "        a story, poem, or picture that can be interpreted to reveal a hidden meaning, typically a moral or political one.\n",
        "- Dialects\n",
        "        a particular form of a language which is peculiar to a specific region or social group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFSr4lDxl6iC"
      },
      "source": [
        "## Corpus\n",
        "\n",
        "In linguistics, a corpus (plural corpora) or text corpus is a large and structured set of texts (nowadays usually electronically stored and processed).\n",
        "\n",
        "<table border=\"1\" class=\"docutils\" id=\"tab-corpora\">\n",
        "<colgroup>\n",
        "<col width=\"31%\">\n",
        "<col width=\"18%\">\n",
        "<col width=\"51%\">\n",
        "</colgroup>\n",
        "<thead valign=\"bottom\">\n",
        "<tr><th class=\"head\">Corpus</th>\n",
        "<th class=\"head\">Compiler</th>\n",
        "<th class=\"head\">Contents</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody valign=\"top\">\n",
        "<tr><td>Brown Corpus</td>\n",
        "<td>Francis, Kucera</td>\n",
        "<td>15 genres, 1.15M words, tagged, categorized</td>\n",
        "</tr>\n",
        "<tr><td>CESS Treebanks</td>\n",
        "<td>CLiC-UB</td>\n",
        "<td>1M words, tagged and parsed (Catalan, Spanish)</td>\n",
        "</tr>\n",
        "<tr><td>Chat-80 Data Files</td>\n",
        "<td>Pereira &amp; Warren</td>\n",
        "<td>World Geographic Database</td>\n",
        "</tr>\n",
        "<tr><td>CMU Pronouncing Dictionary</td>\n",
        "<td>CMU</td>\n",
        "<td>127k entries</td>\n",
        "</tr>\n",
        "<tr><td>CoNLL 2000 Chunking Data</td>\n",
        "<td>CoNLL</td>\n",
        "<td>270k words, tagged and chunked</td>\n",
        "</tr>\n",
        "<tr><td>CoNLL 2002 Named Entity</td>\n",
        "<td>CoNLL</td>\n",
        "<td>700k words, pos- and named-entity-tagged (Dutch, Spanish)</td>\n",
        "</tr>\n",
        "<tr><td>CoNLL 2007 Dependency Treebanks (sel)</td>\n",
        "<td>CoNLL</td>\n",
        "<td>150k words, dependency parsed (Basque, Catalan)</td>\n",
        "</tr>\n",
        "<tr><td>Dependency Treebank</td>\n",
        "<td>Narad</td>\n",
        "<td>Dependency parsed version of Penn Treebank sample</td>\n",
        "</tr>\n",
        "<tr><td>Floresta Treebank</td>\n",
        "<td>Diana Santos et al</td>\n",
        "<td>9k sentences, tagged and parsed (Portuguese)</td>\n",
        "</tr>\n",
        "<tr><td>Gazetteer Lists</td>\n",
        "<td>Various</td>\n",
        "<td>Lists of cities and countries</td>\n",
        "</tr>\n",
        "<tr><td>Genesis Corpus</td>\n",
        "<td>Misc web sources</td>\n",
        "<td>6 texts, 200k words, 6 languages</td>\n",
        "</tr>\n",
        "<tr><td>Gutenberg (selections)</td>\n",
        "<td>Hart, Newby, et al</td>\n",
        "<td>18 texts, 2M words</td>\n",
        "</tr>\n",
        "<tr><td>Inaugural Address Corpus</td>\n",
        "<td>CSpan</td>\n",
        "<td>US Presidential Inaugural Addresses (1789-present)</td>\n",
        "</tr>\n",
        "<tr><td>Indian POS-Tagged Corpus</td>\n",
        "<td>Kumaran et al</td>\n",
        "<td>60k words, tagged (Bangla, Hindi, Marathi, Telugu)</td>\n",
        "</tr>\n",
        "<tr><td>MacMorpho Corpus</td>\n",
        "<td>NILC, USP, Brazil</td>\n",
        "<td>1M words, tagged (Brazilian Portuguese)</td>\n",
        "</tr>\n",
        "<tr><td>Movie Reviews</td>\n",
        "<td>Pang, Lee</td>\n",
        "<td>2k movie reviews with sentiment polarity classification</td>\n",
        "</tr>\n",
        "<tr><td>Names Corpus</td>\n",
        "<td>Kantrowitz, Ross</td>\n",
        "<td>8k male and female names</td>\n",
        "</tr>\n",
        "<tr><td>NIST 1999 Info Extr (selections)</td>\n",
        "<td>Garofolo</td>\n",
        "<td>63k words, newswire and named-entity SGML markup</td>\n",
        "</tr>\n",
        "<tr><td>NPS Chat Corpus</td>\n",
        "<td>Forsyth, Martell</td>\n",
        "<td>10k IM chat posts, POS-tagged and dialogue-act tagged</td>\n",
        "</tr>\n",
        "<tr><td>PP Attachment Corpus</td>\n",
        "<td>Ratnaparkhi</td>\n",
        "<td>28k prepositional phrases, tagged as noun or verb modifiers</td>\n",
        "</tr>\n",
        "<tr><td>Proposition Bank</td>\n",
        "<td>Palmer</td>\n",
        "<td>113k propositions, 3300 verb frames</td>\n",
        "</tr>\n",
        "<tr><td>Question Classification</td>\n",
        "<td>Li, Roth</td>\n",
        "<td>6k questions, categorized</td>\n",
        "</tr>\n",
        "<tr><td>Reuters Corpus</td>\n",
        "<td>Reuters</td>\n",
        "<td>1.3M words, 10k news documents, categorized</td>\n",
        "</tr>\n",
        "<tr><td>Roget's Thesaurus</td>\n",
        "<td>Project Gutenberg</td>\n",
        "<td>200k words, formatted text</td>\n",
        "</tr>\n",
        "<tr><td>RTE Textual Entailment</td>\n",
        "<td>Dagan et al</td>\n",
        "<td>8k sentence pairs, categorized</td>\n",
        "</tr>\n",
        "<tr><td>SEMCOR</td>\n",
        "<td>Rus, Mihalcea</td>\n",
        "<td>880k words, part-of-speech and sense tagged</td>\n",
        "</tr>\n",
        "<tr><td>Senseval 2 Corpus</td>\n",
        "<td>Pedersen</td>\n",
        "<td>600k words, part-of-speech and sense tagged</td>\n",
        "</tr>\n",
        "<tr><td>Shakespeare texts (selections)</td>\n",
        "<td>Bosak</td>\n",
        "<td>8 books in XML format</td>\n",
        "</tr>\n",
        "<tr><td>State of the Union Corpus</td>\n",
        "<td>CSPAN</td>\n",
        "<td>485k words, formatted text</td>\n",
        "</tr>\n",
        "<tr><td>Stopwords Corpus</td>\n",
        "<td>Porter et al</td>\n",
        "<td>2,400 stopwords for 11 languages</td>\n",
        "</tr>\n",
        "<tr><td>Swadesh Corpus</td>\n",
        "<td>Wiktionary</td>\n",
        "<td>comparative wordlists in 24 languages</td>\n",
        "</tr>\n",
        "<tr><td>Switchboard Corpus (selections)</td>\n",
        "<td>LDC</td>\n",
        "<td>36 phonecalls, transcribed, parsed</td>\n",
        "</tr>\n",
        "<tr><td>Univ Decl of Human Rights</td>\n",
        "<td>United Nations</td>\n",
        "<td>480k words, 300+ languages</td>\n",
        "</tr>\n",
        "<tr><td>Penn Treebank (selections)</td>\n",
        "<td>LDC</td>\n",
        "<td>40k words, tagged and parsed</td>\n",
        "</tr>\n",
        "<tr><td>TIMIT Corpus (selections)</td>\n",
        "<td>NIST/LDC</td>\n",
        "<td>audio files and transcripts for 16 speakers</td>\n",
        "</tr>\n",
        "<tr><td>VerbNet 2.1</td>\n",
        "<td>Palmer et al</td>\n",
        "<td>5k verbs, hierarchically organized, linked to WordNet</td>\n",
        "</tr>\n",
        "<tr><td>Wordlist Corpus</td>\n",
        "<td>OpenOffice.org et al</td>\n",
        "<td>960k words and 20k affixes for 8 languages</td>\n",
        "</tr>\n",
        "<tr><td>WordNet 3.0 (English)</td>\n",
        "<td>Miller, Fellbaum</td>\n",
        "<td>145k synonym sets</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "\n",
        "\n",
        "</table>\n",
        "\n",
        "[Source: Natural Language Processing with Python,Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch02.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTM27zvKl6iF"
      },
      "source": [
        "### Stop Words\n",
        "\n",
        "Stop words are generally the most common words in a language; there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "batdqTpzl6iG",
        "outputId": "db24cc90-0259-47d6-9995-e38e94deeca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: ignore\n",
            "  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "execution_count": 30,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.warn(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8HxI6Drl6iH"
      },
      "source": [
        "### WordNet\n",
        "WordNet is **a large lexical database of English**. Nouns, verbs, adjectives and adverbs are grouped into **sets of cognitive synonyms (synsets)**, each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. \n",
        "\n",
        "<img src=\"https://www.nltk.org/images/wordnet-hierarchy.png\">\n",
        "\n",
        "[Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch02.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbu7MdDkl6iH",
        "outputId": "dbb4e7d4-b707-4b9d-8d5a-612901d04449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Synset('ambulance.n.01')\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "motorcar = wordnet.synset('car.n.01')\n",
        "types_of_motorcar = motorcar.hyponyms()\n",
        "print(types_of_motorcar[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps1ilO-Fl6iI",
        "outputId": "bb6af98d-fad2-478b-c950-966eab52d422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap', 'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon', 'wagon']\n"
          ]
        }
      ],
      "source": [
        "print(sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgcTHt4cl6iI"
      },
      "source": [
        "### Gutenberg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOe3uMrgl6iI",
        "outputId": "5da8caf6-e375-418a-b4fd-34d3f06a4378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "execution_count": 33,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KuxB2AcYl6iJ",
        "outputId": "d5bc74fe-e1ac-4c7d-d79b-df22251cdc3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[Poems by William Blake 1789]\\n\\n \\nSONGS OF INNOCENCE AND OF EXPERIENCE\\nand THE BOOK of THEL\\n\\n\\n SONGS '"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gutenberg.raw('blake-poems.txt')[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swd2Hs_1l6iJ"
      },
      "source": [
        "### Web Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGbNRNVll6iJ"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import webtext\n",
        "nltk.download('webtext')\n",
        "webtext.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1LmdQmYl6iK"
      },
      "outputs": [],
      "source": [
        "webtext.sents('firefox.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxMivvtKl6iK"
      },
      "source": [
        "### Brown Text\n",
        "\n",
        "15 genres, 1.15M words, tagged, categorized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A95hJ8JHl6iK",
        "outputId": "7339b97d-a254-427f-b09f-635104cb3db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "execution_count": 37,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "brown.categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJeR5taol6iL",
        "outputId": "4fd08df0-0071-4d93-95a6-eaa02d7519a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brown.words(categories='news')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4gVD2mRl6iL"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "           (genre, word)\n",
        "           for genre in brown.categories()\n",
        "           for word in brown.words(categories=genre))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSKnIarQl6iM",
        "outputId": "b7f66a49-5bb5-4062-e8ab-614eddb776cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  can could   may might  must  will \n",
            "           news    93    86    66    38    50   389 \n",
            "       religion    82    59    78    12    54    71 \n",
            "        hobbies   268    58   131    22    83   264 \n",
            "science_fiction    16    49     4    12     8    16 \n",
            "        romance    74   193    11    51    45    43 \n",
            "          humor    16    30     8     8     9    13 \n"
          ]
        }
      ],
      "source": [
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "cfd.tabulate(conditions=genres, samples=modals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgGcj9xxl6iN"
      },
      "source": [
        "### Pronouncing Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbzQpwNpl6iN",
        "outputId": "93e98870-da38-4b65-dc51-eb72db2bd603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('abreu', ['AH0', 'B', 'R', 'UW1']),\n",
              " ('abridge', ['AH0', 'B', 'R', 'IH1', 'JH']),\n",
              " ('abridged', ['AH0', 'B', 'R', 'IH1', 'JH', 'D']),\n",
              " ('abridgement', ['AH0', 'B', 'R', 'IH1', 'JH', 'M', 'AH0', 'N', 'T']),\n",
              " ('abridges', ['AH0', 'B', 'R', 'IH1', 'JH', 'AH0', 'Z']),\n",
              " ('abridging', ['AH0', 'B', 'R', 'IH1', 'JH', 'IH0', 'NG']),\n",
              " ('abril', ['AH0', 'B', 'R', 'IH1', 'L']),\n",
              " ('abroad', ['AH0', 'B', 'R', 'AO1', 'D']),\n",
              " ('abrogate', ['AE1', 'B', 'R', 'AH0', 'G', 'EY2', 'T']),\n",
              " ('abrogated', ['AE1', 'B', 'R', 'AH0', 'G', 'EY2', 'T', 'IH0', 'D'])]"
            ]
          },
          "execution_count": 41,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('cmudict')\n",
        "entries = nltk.corpus.cmudict.entries()\n",
        "entries[300:310]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMOeAZYal6iN"
      },
      "source": [
        "### Comparative Wordlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAHuNIvWl6iO",
        "outputId": "fdfb71ec-cd6d-4f0a-ce93-f3f196bcf5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]   Package swadesh is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr']"
            ]
          },
          "execution_count": 42,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import swadesh\n",
        "nltk.download('swadesh')\n",
        "swadesh.fileids()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJNqJ6qHl6iO",
        "outputId": "52213713-a534-4f1e-d3db-bffbd380c73b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(207, ('nous', 'we'))"
            ]
          },
          "execution_count": 43,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fr2en = swadesh.entries(['fr', 'en'])\n",
        "len(fr2en) , fr2en[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69x69ogdl6iO"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzdrmuGDl6iO"
      },
      "source": [
        "### Concept\n",
        "- Split on white space (in most languages)\n",
        "- **Regular expressions** and **finite state automata** are often used\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlTxB96yl6iP"
      },
      "source": [
        "### Issues in Tokenization\n",
        "\n",
        "Consider: “There’s a moon in the sky.  It’s called The Moon.\" (B52's)\n",
        "\n",
        "- Special cases: Names (particularly multi-word), initials, hyphenated words, abbreviations, special forms (dates, phone numbers, URLs, etc.)\n",
        "- Punctuation\n",
        "- Contractions: is \"isn’t\" one word or two (many tokenizers treat as two)\n",
        "- Named Entities\n",
        "- Rare words\n",
        "- Stop words\n",
        "\n",
        "- Other languages\n",
        "  - German: Compounds words such as schadenfreude\n",
        "  - Chinese: average of about 2 symbol/word; greedy readings work quite well but there are better algos\n",
        "  - Japanese: kanji, hirigana, katakana, romanji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuWKezghl6iP"
      },
      "source": [
        "### Popular Python tokenizers:\n",
        "  - NLTK\n",
        "  - SpaCy\n",
        "  - Keras\n",
        "  - CoreNLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH3_NYPWl6iP"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lo5zFVgl6iP",
        "outputId": "c09f01f9-cfb7-461b-ce06-cb494054696a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['DENNIS: Listen, strange women lying in ponds distributing swords\\nis no basis for a system of government.', 'Supreme executive power derives from\\na mandate from the masses, not from some farcical aquatic ceremony.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
        "is no basis for a system of government.  Supreme executive power derives from\n",
        "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
        "tokens = sent_tokenize(raw)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dot_CyBAl6iP",
        "outputId": "a6454631-9b65-4c8a-f762-1209241dff97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(raw)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ4lyLf3l6iQ"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "The concept is to replace similar words with a single token and reduce word vector size. This task is a kind of dimensionality reduction. It reduces the processing cost and the likelihood of overfitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVK5N7gWl6iQ"
      },
      "source": [
        "### Normalization Techniques\n",
        "- Case folding: Force all lower case \n",
        "  - Can make named entity resolution more difficult\n",
        "  - Becoming less common as a result\n",
        "- Stemming: Crude chopping of affixes\n",
        "  - e.g. remove -s or -ing at end\n",
        "  - Can cut vocabulary size in half (or more if aggressive)\n",
        "  - Many algoritms: Porter’s is most common English stemmer and has a lot of knowledge of English hardcoded in it\n",
        "  - Useful for search where we are looking for similar, not exact matches (will improve recall but reduce precision)\n",
        "- Lemmatization: Extraction of the base form\n",
        "  - e.g. “are”, “am”, “is” replaced with “be”\n",
        "  - Better for most applications than stemmers which might take “better” and convert it to “bet”\n",
        "- Hashtag expansions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAUhpADyl6iQ"
      },
      "source": [
        "### When to Use Stemming vs. Lemmatization\n",
        "- If you want the recall benefit of stemming try putting a lemmatizer before the stemmer\n",
        "- NLTK lemmatizer uses the Princeton WordNet graph of word meanings\n",
        "- Newer packages like SpaCy don’t provide a stemmer, only. a lemmatizer\n",
        "- Stemmers and lemmatizers (like stop words) are being less used all the time as computers become more powerful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pasbh5oLl6iQ"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRma93Jgl6iR",
        "outputId": "3fc2bf00-645a-4c5c-c4e0-441a6d32bafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
          ]
        }
      ],
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "print([porter.stem(t) for t in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EwLigKhl6iR",
        "outputId": "991c2205-cc5b-4b8e-9ce9-c12f1f1ffb63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
          ]
        }
      ],
      "source": [
        "lancaster = nltk.LancasterStemmer()\n",
        "print([lancaster.stem(t) for t in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih1vwpWWoisd",
        "outputId": "e0e2b932-2d50-4f0a-d646-89adaae57c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZTnC8tBl6iS",
        "outputId": "dde90217-f25f-40b0-e09d-f5fbfa36a87e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('DENNIS', 'DENNIS'),\n",
              " ('Listen', 'Listen'),\n",
              " ('strange', 'strange'),\n",
              " ('women', 'woman'),\n",
              " ('lying', 'lie'),\n",
              " ('ponds', 'pond'),\n",
              " ('distributing', 'distribute'),\n",
              " ('swords', 'sword'),\n",
              " ('is', 'be'),\n",
              " ('basis', 'basis'),\n",
              " ('system', 'system'),\n",
              " ('government', 'government'),\n",
              " ('Supreme', 'Supreme'),\n",
              " ('executive', 'executive'),\n",
              " ('power', 'power'),\n",
              " ('derives', 'derive'),\n",
              " ('mandate', 'mandate'),\n",
              " ('masses', 'mass'),\n",
              " ('not', 'not'),\n",
              " ('farcical', 'farcical'),\n",
              " ('aquatic', 'aquatic'),\n",
              " ('ceremony', 'ceremony')]"
            ]
          },
          "execution_count": 49,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Getting the part of speech of a word\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "        print(tag)\n",
        "\n",
        "tagged_sent = nltk.pos_tag(tokens) # Gets part of speech\n",
        "lmtzr = WordNetLemmatizer()\n",
        "[(x[0],lmtzr.lemmatize(x[0], get_wordnet_pos(x[1])))\n",
        " for x in tagged_sent if get_wordnet_pos(x[1]) is not None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pZKnKVDl6iT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i4ie582l6iT"
      },
      "source": [
        "## Annotators \n",
        "\n",
        "<img src=\"https://stanfordnlp.github.io/CoreNLP/assets/images/pipeline.png\">\n",
        "\n",
        "[source CoreNLP Website](https://stanfordnlp.github.io/CoreNLP/)\n",
        "\n",
        "\n",
        "you can try different annotators [here](http://corenlp.run/)\n",
        "\n",
        "For spaCy you should run following commands: \n",
        "\n",
        "* `pip install spacy`\n",
        "* `python -m spacy download en_core_web_sm`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hX6L7KLl6iT"
      },
      "source": [
        "### POS (Part Of Speech) Tagger \n",
        "\n",
        "Part-of-speech tagging (POS tagging or PoS tagging or POST), also called **grammatical tagging or word-category disambiguation**, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.[source](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=19I5kJPRfQ6OdnWVHr9BqYWYB11B9Mrcl\">\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvqK3uzZl6iT",
        "outputId": "9bbdea5c-c82f-45eb-e2ab-6ff2c5c701bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('refuse', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('permit', 'VB'),\n",
              " ('us', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('obtain', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('refuse', 'NN'),\n",
              " ('permit', 'NN')]"
            ]
          },
          "execution_count": 50,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PgySR4yl6iU",
        "outputId": "ecaf2fc8-c354-42a5-e1c4-e5add9549256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n"
          ]
        }
      ],
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset('DT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJsqxtoPl6iU"
      },
      "source": [
        "### Name Entity Recognition \n",
        "\n",
        "**Named-entity recognition (NER)** is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text. [source](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
        "\n",
        "In information extraction, a **named entity** is a **real-world object**, such as persons, locations, organizations, products, etc., that can be denoted with a proper name. It can be abstract or have a physical existence. Examples of named entities include Barack Obama, New York City, Volkswagen Golf, or anything else that can be named. [source](https://en.wikipedia.org/wiki/Named_entity)\n",
        "\n",
        "<img src=\"https://stanfordnlp.github.io/CoreNLP/assets/images/ner.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFoFepJql6iU"
      },
      "source": [
        "### Constituent Parsing\n",
        "\n",
        "Constituent parsing is the task of **recognizing a sentence and assigning a syntactic structure** to it.[source](https://stanfordnlp.github.io/CoreNLP/parse.html) \n",
        "\n",
        "\n",
        "[Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch07.html)\n",
        "\n",
        "One of the common problems is **Ubiquitous Ambiguity**. Please consider example below\n",
        "\n",
        "\n",
        "\n",
        "[Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch07.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_lk5Iupl6iV"
      },
      "source": [
        "### Dependency Parsing \n",
        "\n",
        "Dependency parsing is the task of extracting a dependency parse of a sentence that represents its grammatical structure and **defines the relationships between “head” words and words**, which modify those heads.[source](http://nlpprogress.com/english/dependency_parsing.html)\n",
        "\n",
        "<img src=\"https://stanfordnlp.github.io/CoreNLP/assets/images/depparse.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lu2wve_l6iV"
      },
      "source": [
        "### Coreference Resolution\n",
        "\n",
        "Coreference resolution is the task of finding **all expressions that refer to the same entity** in a text. \n",
        "\n",
        "<img src=\"https://nlp.stanford.edu/projects/corefexample.png\">\n",
        "\n",
        "[source:nlpCore website](https://nlp.stanford.edu/projects/coref.shtml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Zj9hQgl6iV"
      },
      "source": [
        "### Open information extraction (open IE) \n",
        "\n",
        "**Open information extraction (open IE)** refers to the **extraction of relation tuples**, typically binary relations, from plain text, such as (Mark Zuckerberg; founded; Facebook).[source](https://nlp.stanford.edu/software/openie.html)\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq1IJJEfl6iV"
      },
      "source": [
        "##  Regular Expressions\n",
        "\n",
        "A **regular expression** or **regex** is a **sequence of characters that define a search pattern**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRDnpCHpl6iW"
      },
      "source": [
        "- Helps you find and match the patterns in text\n",
        "    - Finding all the email addresses in webpage\n",
        "    - Email addresses are strings that has exactly one @ sign, and at least one . in the part after the @\n",
        "    - Based on the above description (which is too general but mostly enough for basic check) the regular expression for an email address is as follows\n",
        "    - Email pattern: **[^@]+@[^@]+\\.[^@]+**\n",
        "    - The above pattern means some at least one character which is not @ ([^@]+), followed by an @ sign, followed by at least one character which is not @ ([^@]+), followed by a single . , and again followed by at least one character which is not @ ([^@]+)\n",
        "    - A more comprehensive pattern that does not allow spaces inside email adresses is as follows:\n",
        "    - Email pattern: **[^@|\\s]+@[^@]+\\.[^@|\\s]+**\n",
        "    \n",
        "- Some Regular expression syntax examples \n",
        "    - **.**\tMatches any character\n",
        "    - **^abc**\tMatches some pattern abc at start of a string\n",
        "    - **abc$**\tMatches some pattern abc at the end of a string\n",
        "    - **[abc]**\tMatches one of a set of characters\n",
        "    - **[A-Z0-9]**\tMatches one of a range of character\n",
        "    - **ed|ing|s**\tMatches one of the specified strings\n",
        "    - **\\***\tMatches zero or more of the previous item\n",
        "    - **\\+**\tMatches one or more of the previous item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAkWb2ILl6iW"
      },
      "source": [
        "###  Regular Expressions in Python\n",
        "\n",
        "- Take a crash course to learn the most commonly used syntax for writing the patterns\n",
        "    - [Regex tutorial — A quick cheatsheet by examples](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)\n",
        "    - [Python Regex Cheatsheet](https://www.debuggex.com/cheatsheet/regex/python)\n",
        "    - In Python **re** library can be used. It has many methods and the following methos are more common\n",
        "        - re.match()\n",
        "        - re.search()\n",
        "    - The match() function only checks if the RE matches at the beginning of the string while search() will scan forward through the string for a match. It’s important to keep this distinction in mind. Remember, match() will only report a successful match which will start at 0; if the match wouldn’t start at zero, match() will not report it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFITDmTYl6iW",
        "outputId": "e754481b-7e7c-47e3-d606-dea8cb897a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, 5)\n",
            "None\n",
            "(0, 5)\n",
            "(2, 7)\n"
          ]
        }
      ],
      "source": [
        "# Let's see how we can make use of re library to extract part of text\n",
        "import re\n",
        "print(re.match('super', 'superstition').span())\n",
        "print(re.match('super', 'insuperable'))\n",
        "print(re.search('super', 'superstition').span())\n",
        "print(re.search('super', 'insuperable').span())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfEtcgusl6iW"
      },
      "source": [
        "### Finding email addresses in a text document using Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ic68EUl6iX"
      },
      "source": [
        "The following code shows how we can use **re** library to extract all the **valid** email addresses from a source docuements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG3emde3l6iX",
        "outputId": "c72fc82c-aee1-4503-8137-130717986259"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['example@me.com', 'me@example.com']"
            ]
          },
          "execution_count": 53,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Finding email addresses in a text file\n",
        "import re\n",
        "string=\"Hello friend, You can send me an email either to example@me.com or to me@example.com or me[at]gmail[dot]com\"\n",
        "# findall returns all non-overlapping matches of pattern in string, as a list of strings. \n",
        "res = re.findall(\"[^@|\\s]+@[^@]+\\.[^@|\\s]+\",string) \n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQj69uK9l6iX"
      },
      "source": [
        "### Relation Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdxMYIJnl6iY",
        "outputId": "1c1bc9f9-2937-4e0e-ba10-72ceb5257e1f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/ieer.zip.\n",
            "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
            "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
            "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
            "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
            "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
            "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
            "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
            "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
            "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
            "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
            "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
            "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
            "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "nltk.download('ieer')\n",
        "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
        "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
        "     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
        "        print(nltk.sem.rtuple(rel))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4nq_b8xl6iY"
      },
      "source": [
        "## Language Models\n",
        "\n",
        "A statistical language model is a **probability distribution over sequences of words**. Given such a sequence, say of length m, it assigns a probability ${\\displaystyle P(w_{1},\\ldots ,w_{m})}$ to the whole sequence.\n",
        "\n",
        "The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases \"recognize speech\" and \"wreck a nice beach\" sound similar, but mean different things.\n",
        "\n",
        "[source](https://en.wikipedia.org/wiki/Language_model)\n",
        "- What is the probability of the next word given what we’ve seen so far?\n",
        "- Can’t just count instances: too many sentences; never have enough data\n",
        "- So we use a simplifying assumption: The Markov property (only the previous few words matter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxh-r-39l6iY"
      },
      "source": [
        "## n-Grams\n",
        "\n",
        "In the fields of computational linguistics and probability, an n-gram is a **continuous sequence of n items from a given sample of text or speech**.[source](https://en.wikipedia.org/wiki/N-gram)\n",
        "\n",
        "- The Distributional Hypothesis (1950’s): There is a link between how words are distributed and what they mean\n",
        "- Sapir-Whorf Hypothesis: The structure of a language determines a native speaker's perception and categorization of experience\n",
        "- Collocations: Words that go together to express a concept\n",
        "- Concordances: An alpabetical index that shows words in their context\n",
        "- n-grams are sequences of words found in natural language text\n",
        "- Useful for translation, spelling correction, speech recognition, question answering\n",
        "- Consider the meaning of “not old” vs. not and old individually: perhaps we should add “not old” to our vocabulary if it occurs\n",
        "- We can add 2, 3, etc.-grams\n",
        "- Can have letter n-grams as well (e.g. used internally in DBMS’s for wildcard lookups) but for our purposes we are only concerned with word n-grams\n",
        "- No point in trying to generate all possible n-grams in advance (combinatorial explosion) so we are only interested in the ones that actually occur in our corpus\n",
        "- Consider very rare collocations: Can’t determine their frequency of occurrence so not useful for classification problems\n",
        "- Consider very common collocations (e.g. “is a”): Carry almost no information (but useful for language detection)\n",
        "- A vocabulary of about 20,000 words is sufficient to track 95% of words in a corpus of tweets, blog posts and news article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coBkH2-9l6iY",
        "outputId": "f25878d1-0430-4f31-afb1-93f5b08d31a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('DENNIS:', 'Listen,')\n",
            "('Listen,', 'strange')\n",
            "('strange', 'women')\n",
            "('women', 'lying')\n",
            "('lying', 'in')\n",
            "('in', 'ponds')\n",
            "('ponds', 'distributing')\n",
            "('distributing', 'swords')\n",
            "('swords', 'is')\n",
            "('is', 'no')\n",
            "('no', 'basis')\n",
            "('basis', 'for')\n",
            "('for', 'a')\n",
            "('a', 'system')\n",
            "('system', 'of')\n",
            "('of', 'government')\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = 'DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government'\n",
        "n = 2\n",
        "sixgrams = ngrams(sentence.split(), n)\n",
        "\n",
        "for grams in sixgrams:\n",
        "    print(grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-cbhcThl6iY"
      },
      "source": [
        "## Representing Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upIRT3u5l6iZ"
      },
      "source": [
        "- Primary \n",
        "    - One Hot Encoding \n",
        "    \n",
        "- Advanced Word Embeddings\n",
        "    - WordToVec\n",
        "    - Glove\n",
        "    - TagLM\n",
        "    - ELMO\n",
        "    - GPT\n",
        "    - BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKMRUkENl6iZ"
      },
      "source": [
        "#### One Hot Encoding  Example\n",
        "\n",
        "Following you can find an example for hot-encoding with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWWDHUP-l6iZ",
        "outputId": "f70a2c6d-d11d-4d5c-9d7e-8c6d1ab7c34f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[127,\n",
              " 224,\n",
              " 88,\n",
              " 198,\n",
              " 336,\n",
              " 296,\n",
              " 4,\n",
              " 205,\n",
              " 175,\n",
              " 344,\n",
              " 177,\n",
              " 235,\n",
              " 202,\n",
              " 226,\n",
              " 39,\n",
              " 334,\n",
              " 206]"
            ]
          },
          "execution_count": 56,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "sentence = 'DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government'\n",
        "\n",
        "tf.keras.preprocessing.text.one_hot(\n",
        "    sentence,\n",
        "    400,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' '\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoPrBPHMl6ia"
      },
      "source": [
        "## Representing Documents\n",
        "### Pre-Deep Learning Approaches\n",
        "- Bag of Words\n",
        "- Bags of n-Grams\n",
        "- TF-IDF vectors\n",
        "- topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biW-i6-Zl6ia"
      },
      "source": [
        "### Bag of Words or n-Grams\n",
        "\n",
        "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words,\n",
        "\n",
        "- We would like to model words, n-grams and documents in a way that is amenable to computer processing\n",
        "- First attempt\n",
        "  - One Hot encoding for words and n-grams\n",
        "  - Bag of Words (BOW) encoding for documents: sum or logical-OR of word vectors\n",
        "- Advantages:\n",
        "  - Numeric representation\n",
        "  - Simple to compute\n",
        "  - Easy to interpret and use\n",
        "  - Can compare two documents by comparing their BOW encodings\n",
        "- Disadvantages:\n",
        "  - Loses context and therefore meaning\n",
        "  - Hugely wasteful of memory space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O17VyG-gl6ia",
        "outputId": "9362495e-d81a-44a6-a058-c3cc3829800f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "dataset = api.load(\"text8\")\n",
        "dct = Dictionary(dataset)  # fit dictionary\n",
        "corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LthzA-DDl6ib",
        "outputId": "9884b85f-ef3c-4226-ecea-92ffc09bf495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a 184\n",
            "abacus 1\n",
            "abilities 1\n",
            "ability 3\n",
            "able 7\n",
            "abnormal 1\n",
            "abolished 1\n",
            "abolition 1\n",
            "about 12\n",
            "above 2\n"
          ]
        }
      ],
      "source": [
        "for word_id,count in corpus[0][:10]:\n",
        "    print(dct[word_id], count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQghbqq_l6ib"
      },
      "source": [
        "### TF-IDF\n",
        "In information retrieval, tf–idf or TFIDF, short for **term frequency–inverse document frequency**, is a **numerical statistic** that is intended to **reflect how important a word is to a document in a collection or corpus**.\n",
        "- Let's say we want to divide up a corpus (collection) of documents into similar clusters\n",
        "- How should we decide how similar two documents are?\n",
        "  - How many words they have in common\n",
        "  - How specialized those words are\n",
        "- To capture these two aspects we need two measures for each word in our vocabulary:\n",
        "  - Term Frequency: How frequently the word occurs in each document\n",
        "  - Document Frequency: How often the word occurs in our corpus\n",
        "- We can combine these measures as Term Frequency / Document Frequency (called TF-IDF for Term Frequency, Inverse Document Frequency)\n",
        "- the Document Frequency is usually measured on a log scale\n",
        "\n",
        "\n",
        "$$\n",
        "{\\text { tfidf }(t, d, D)=t f(t, d) \\cdot \\text { idf }(t, D)}$$\n",
        "\n",
        "$$\\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRaCpxygl6ib"
      },
      "outputs": [],
      "source": [
        "model = TfidfModel(corpus)  # fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huM8MFcyl6ib"
      },
      "outputs": [],
      "source": [
        "vector = model[corpus[0]]  # apply model to the first corpus document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPFcIHTal6ib",
        "outputId": "b2dbc1fc-a5d0-4681-c398-9af39989b748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "abacus 0.006704047545684609\n",
            "abilities 0.0030255603220721273\n",
            "ability 0.003156168449586299\n",
            "able 0.0036673470201144674\n",
            "abnormal 0.004575122435127926\n",
            "abolished 0.0028052608258295926\n",
            "abolition 0.004064820137019515\n",
            "about 0.00014963587508918375\n",
            "above 0.0007492665180478759\n",
            "absence 0.004142807322609117\n"
          ]
        }
      ],
      "source": [
        "for word_id,weight in vector[:10]:\n",
        "    print(dct[word_id], weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOkAxthel6ic"
      },
      "source": [
        "### Topic Modeling \n",
        "\n",
        "Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents.\n",
        "\n",
        "#### Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. [source](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png\">\n",
        "\n",
        "α is the parameter of the Dirichlet prior on the per-document topic distributions\n",
        "\n",
        "β is the parameter of the Dirichlet prior on the per-topic word distribution\n",
        "\n",
        "${\\displaystyle \\theta _{i}}\\theta _{i}$ is the topic distribution for document i\n",
        "\n",
        "${\\displaystyle \\varphi _{k}}\\varphi _{k}$ is the word distribution for topic k\n",
        "\n",
        "${\\displaystyle z_{ij}}z_{ij}$ is the topic for the j-th word in document i\n",
        "\n",
        "${\\displaystyle w_{ij}}w_{ij}$ is the specific word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sdS2D5Bl6ic"
      },
      "outputs": [],
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "lda = LdaModel(corpus, num_topics=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhFF3K81l6ic",
        "outputId": "99ecf73d-81ec-4069-fba9-e5cdfcdc3790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, '0.053*\"2240\" + 0.032*\"1561\" + 0.025*\"119\" + 0.022*\"1129\" + 0.019*\"2276\" + 0.018*\"2517\" + 0.017*\"0\" + 0.014*\"1573\" + 0.013*\"1523\" + 0.010*\"1215\"'), (1, '0.042*\"2240\" + 0.032*\"1561\" + 0.023*\"1573\" + 0.021*\"119\" + 0.021*\"1129\" + 0.020*\"0\" + 0.016*\"1523\" + 0.016*\"2276\" + 0.012*\"1215\" + 0.010*\"2328\"'), (2, '0.058*\"2240\" + 0.039*\"1561\" + 0.025*\"1573\" + 0.021*\"0\" + 0.018*\"1129\" + 0.018*\"2276\" + 0.014*\"119\" + 0.013*\"2517\" + 0.011*\"1523\" + 0.011*\"2328\"'), (3, '0.061*\"2240\" + 0.030*\"1561\" + 0.027*\"1573\" + 0.025*\"119\" + 0.021*\"0\" + 0.020*\"1523\" + 0.019*\"1129\" + 0.018*\"2517\" + 0.016*\"2276\" + 0.009*\"2328\"'), (4, '0.068*\"2240\" + 0.036*\"1561\" + 0.027*\"119\" + 0.025*\"1573\" + 0.025*\"2276\" + 0.020*\"1129\" + 0.014*\"2517\" + 0.014*\"1215\" + 0.014*\"0\" + 0.013*\"1523\"'), (5, '0.059*\"2240\" + 0.034*\"1561\" + 0.025*\"119\" + 0.025*\"1573\" + 0.022*\"0\" + 0.020*\"1523\" + 0.019*\"1129\" + 0.016*\"2517\" + 0.015*\"2276\" + 0.012*\"2328\"'), (6, '0.053*\"2240\" + 0.029*\"1561\" + 0.022*\"1129\" + 0.021*\"0\" + 0.021*\"2276\" + 0.021*\"119\" + 0.017*\"2517\" + 0.016*\"1573\" + 0.012*\"1523\" + 0.011*\"2328\"'), (7, '0.056*\"2240\" + 0.030*\"1561\" + 0.026*\"119\" + 0.024*\"0\" + 0.022*\"1573\" + 0.016*\"1129\" + 0.016*\"2517\" + 0.014*\"1523\" + 0.014*\"2276\" + 0.014*\"2328\"'), (8, '0.050*\"2240\" + 0.041*\"1561\" + 0.030*\"119\" + 0.030*\"1129\" + 0.023*\"1573\" + 0.023*\"0\" + 0.017*\"2276\" + 0.015*\"2517\" + 0.014*\"1523\" + 0.014*\"2328\"'), (9, '0.070*\"2240\" + 0.031*\"1561\" + 0.023*\"119\" + 0.022*\"1129\" + 0.020*\"0\" + 0.020*\"2276\" + 0.016*\"1573\" + 0.015*\"2517\" + 0.012*\"1523\" + 0.012*\"2328\"'), (10, '0.065*\"2240\" + 0.037*\"1561\" + 0.032*\"1573\" + 0.025*\"119\" + 0.021*\"1129\" + 0.020*\"0\" + 0.017*\"2517\" + 0.017*\"2276\" + 0.012*\"1523\" + 0.010*\"1215\"'), (11, '0.048*\"2240\" + 0.034*\"1561\" + 0.029*\"119\" + 0.026*\"1573\" + 0.021*\"1523\" + 0.020*\"2517\" + 0.017*\"0\" + 0.016*\"1129\" + 0.015*\"2276\" + 0.014*\"2328\"'), (12, '0.056*\"2240\" + 0.031*\"1561\" + 0.023*\"1129\" + 0.021*\"119\" + 0.018*\"1573\" + 0.018*\"0\" + 0.017*\"2517\" + 0.016*\"1523\" + 0.014*\"2276\" + 0.012*\"2328\"'), (13, '0.059*\"2240\" + 0.034*\"1561\" + 0.022*\"1573\" + 0.020*\"0\" + 0.019*\"119\" + 0.019*\"1129\" + 0.016*\"2276\" + 0.012*\"2517\" + 0.011*\"1215\" + 0.010*\"1523\"'), (14, '0.053*\"2240\" + 0.030*\"1561\" + 0.023*\"119\" + 0.022*\"1573\" + 0.020*\"0\" + 0.019*\"1129\" + 0.017*\"2276\" + 0.015*\"1523\" + 0.011*\"2328\" + 0.010*\"2517\"'), (15, '0.078*\"2240\" + 0.026*\"1561\" + 0.026*\"1129\" + 0.022*\"1573\" + 0.021*\"2276\" + 0.018*\"0\" + 0.018*\"119\" + 0.015*\"1523\" + 0.014*\"2517\" + 0.013*\"2328\"'), (16, '0.051*\"2240\" + 0.034*\"1561\" + 0.019*\"119\" + 0.018*\"1129\" + 0.016*\"1573\" + 0.016*\"2276\" + 0.016*\"0\" + 0.015*\"1523\" + 0.011*\"2517\" + 0.009*\"2328\"'), (17, '0.056*\"2240\" + 0.034*\"1561\" + 0.028*\"119\" + 0.020*\"0\" + 0.018*\"2517\" + 0.016*\"1573\" + 0.016*\"2276\" + 0.014*\"2328\" + 0.013*\"1129\" + 0.010*\"1215\"'), (18, '0.065*\"2240\" + 0.035*\"1561\" + 0.025*\"1129\" + 0.023*\"1573\" + 0.022*\"119\" + 0.020*\"2276\" + 0.019*\"0\" + 0.015*\"2517\" + 0.012*\"1215\" + 0.011*\"1523\"'), (19, '0.060*\"2240\" + 0.043*\"1561\" + 0.027*\"1573\" + 0.026*\"1129\" + 0.024*\"119\" + 0.017*\"2276\" + 0.016*\"1523\" + 0.016*\"0\" + 0.013*\"2517\" + 0.011*\"2328\"')]\n"
          ]
        }
      ],
      "source": [
        "print(lda.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Futx-Pb7l6id",
        "outputId": "6ffc1534-3dc3-47e2-cd4a-7170f5a48d82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(4, 0.22378018),\n",
              " (5, 0.055412404),\n",
              " (7, 0.011996026),\n",
              " (8, 0.06039811),\n",
              " (10, 0.27587456),\n",
              " (15, 0.2914846),\n",
              " (18, 0.046026886)]"
            ]
          },
          "execution_count": 64,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_lda = lda[corpus]\n",
        "doc_lda[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEKJTzItl6id"
      },
      "source": [
        "For running code you should install pyLDAvis beforehand by using `pip install pyLDAvis`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "BtJQLfxal6id",
        "outputId": "3426ce13-67bb-41af-c1c8-bef43ed6bcc8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-fc42def2ed73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ApfmOD4l6id"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda, corpus, dct)\n",
        "vis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faYZyxcvl6ie"
      },
      "source": [
        "## Similarity Measure \n",
        "\n",
        "### Cosine Similarity\n",
        "- We can measure the similarity of vectors (such as TF-IDF vectors) using Cosine Similarity\n",
        "- The more similar the vectors are, the smaller the angle there should be between them\n",
        "- The cosine similarity of two vectors, x and y, is easily calculated using dot product \n",
        "\n",
        "\\begin{align}\n",
        "cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||}\n",
        "\\end{align}\n",
        "- Cosine similarity is a number that runs between 0 (nothing in common) to 1 (identical) for TF-IDF vectors (Note: Here identical means identical TF-IDF, not necessarily identical documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8x_KAU1l6ie"
      },
      "source": [
        "## NLP Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mnUrGr2l6ie"
      },
      "source": [
        "Natural Language processing is usually organized into a pipeline of operations that parse and analyze the text.\n",
        "The figure below shows an example of a NLP pipeline:\n",
        "![image.png](attachment:image.png)\n",
        "Source: Natural Language Processing in Action by Lane, Howard & Hapke"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU3_bZBTl6ie"
      },
      "source": [
        "# The Natural Language Toolkit: NLTK\n",
        "\n",
        "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
        "\n",
        "- Originally developed in 2001 at the University of Pennsylvania\n",
        "- Version 3.3 released in 2018\n",
        "- Natural Language Processing with Python book: https://www.nltk.org/book/  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84LnRLUml6ie"
      },
      "source": [
        "## References\n",
        "- http://nlpprogress.com/english/dependency_parsing.html\n",
        "- https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
        "- https://wordnet.princeton.edu/\n",
        "- https://www.nltk.org/\n",
        "- https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition/tree/master/notebooks\n",
        "- https://en.wikipedia.org/wiki/Regular_expression\n",
        "- Lane, Howard & Hapke. Natural Language Processing in Action. Manning. 2019.\n",
        "- Jurafsky & Martin. Speech and Language Processing, 3rd Ed. https://web.stanford.edu/~jurafsky/slp3/\n",
        "- SpaCy: https://spacy.io/\n",
        "- gensim: https://radimrehurek.com/gensim/\n",
        "- [Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch07.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXliWZFzl6if"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jzdrmuGDl6iO",
        "mlTxB96yl6iP",
        "TuWKezghl6iP",
        "pVK5N7gWl6iQ",
        "kAUhpADyl6iQ",
        "hJsqxtoPl6iU",
        "TFoFepJql6iU",
        "n_lk5Iupl6iV",
        "_Lu2wve_l6iV",
        "S-Zj9hQgl6iV",
        "UAkWb2ILl6iW",
        "AfEtcgusl6iW",
        "fQj69uK9l6iX",
        "lKMRUkENl6iZ",
        "biW-i6-Zl6ia",
        "uQghbqq_l6ib"
      ],
      "name": "06 - Natural Language Processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}